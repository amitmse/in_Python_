------------------------------------------------------------------------------------------------------
# Type of data:
### Nominal / categorical/qualitative /non-parametric:  
		Example	: 	colour,gender. 
		check	: 	frequency each category. 
		Test	: 	Comparison/Difference: 
					- Test for proportion (for one categorical variable)
					- Difference of two proportions
					- Chi-Square test for independence (for two categorical variables)
		Relationship:	Chi-Square test for independence
	
	
### Ordinal : Similar as Nominal
		Example	:	rank, satisfaction.  
		Check	: 	frequency or mean (special case). 
		Test	:	Similar as Nominal
		
		
### Interval / Ratio / quantitative/continuous  : 
		Example	:	number of customers, income,age. 
		Check	: 	Mean ..
		Test	:	Comparison/Difference:
					- Test for a mean / T test (for one continuous variable)
					- Difference of two means (independent samples)
					- Difference of two means(paired T test. Pre and Post scenario)
		Relationship:	Regression Analysis / Correlation (for two continuous variables/for relationship)
			
		
#### One categorical and one continuous: T test (Anova when more than 2 category)
	https://www.youtube.com/watch?v=tfiDu--7Gmg
	
### Imbalanced data
		Imbalanced data refers to those types of datasets where the target class has an uneven 
		distribution of observations, i.e one class label has a very high number of observations 
		and the other has a very low number of observations (rare event i.e., Fraud)
------------------------------------------------------------------------------------------------------		
#### Under-sampling: 
- Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modelling.

#### Over-sampling: 
- On the contrary, oversampling is used when the quantity of data is insufficient. It tries to balance dataset by increasing the size of rare samples. Rather than getting rid of abundant samples, new rare samples are generated by using e.g. repetition, bootstrapping or SMOTE (Synthetic Minority Over-Sampling Technique).

#### Use K-fold Cross-Validation in the right way: 
- Cross-validation should be applied properly while using over-sampling method to address imbalance problems. Keep in mind that over-sampling takes observed rare samples and applies bootstrapping to generate new random data based on a distribution function. If cross-validation is applied after over-sampling, basically what we are doing is overfitting our model to a specific artificial bootstrapping result. That is why cross-validation should always be done before over-sampling the data, just as how feature selection should be implemented. Only by resampling the data repeatedly, randomness can be introduced into the dataset to make sure that there won’t be an overfitting problem.

https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html

#### Resampling:
- Consider testing under-sampling when you have an a lot data 
- Consider testing over-sampling when you don’t have a lot of data 
- Consider testing random and non-random (e.g. stratified) sampling schemes.
- Consider testing different resampled ratios

#### Synthetic Samples
- SMOTE or the Synthetic Minority Over-sampling Technique: SMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances.

#### Borderline SMOTE, SVM-SMOTE

#### Balanced Bagging Classifier: 
- When we try to use a usual classifier to classify an imbalanced dataset, the model favors the majority class due to its larger volume presence. A Balanced Bagging Classifier is the same as a sklearn classifier but with additional balancing.

#### Cluster the abundant class: 
- An elegant approach was proposed by Sergey on Quora. Instead of relying on random samples to cover the variety of the training samples, he suggests clustering the abundant class in r groups, with r being the number of cases in r. For each group, only the medoid (centre of cluster) is kept. The model is then trained with the rare class and the medoids only.


---------------------------------------------------------------------------------------------
### Hypothesis Testing: 
- Hypothesis testing uses data from a sample to draw conclusions about a population parameter or a population probability distribution.  
  It's a statistical method used to determine if there's enough evidence in a sample to reject a null hypothesis in favor of an alternative hypothesis.
  It help to make better decisions.

  Avoiding Misleading Conclusions (Type I and Type II Errors): It helps you avoid jumping to the wrong conclusions.
  For instance, a Type I error could occur if a bank launches a new product thinking it will be a hit, only to find out later that the data misled them.
  A Type II error might happen when a bank overlooks a potentially successful product because their testing wasn’t thorough enough.
  By setting up the right significance level and carefully calculating the p-value, hypothesis testing minimizes the chances of these errors, leading to more accurate results.

  Making Smarter Choices: Evidence-based decisions. Let’s say a bank wants to determine if new strategy will increase revenue.
  By testing the hypothesis using data from similar projects, they can make an informed choice.
  Similarly, a bank might use hypothesis testing to see if a credit limit increse actually improves credit card spending.
  It’s about taking the guesswork out of decisions and relying on solid evidence instead.

  Optimizing Business Tactics: In business, hypothesis testing is invaluable for testing new ideas and strategies before fully committing to them.
  For example, bank company might want to test whether offering upgraded credit card increases spending.
  By using hypothesis testing, bank can compare spend data from customers who received upgraded credit card vs who didn’t.
  This allows bank to base decisions on data, not hunches, reducing the risk of costly mistakes.
  
- Null Hypothesis (H0): This hypothesis states that there is no effect or difference, and it is the hypothesis you attempt to reject with your test.
- Alternative Hypothesis (H1 or Ha): This hypothesis is what you might believe to be true or hope to prove true. It is usually considered the opposite of the null hypothesis.
  
- Significance level: A significance level is used to determine if the null hypothesis is true or not. Often denoted by alpha (α), is the probability of accurately rejecting the null hypothesis. 
  Common choices for α are 0.05 (5%), 0.01 (1%), and 0.10 (10%).
  The significance level is the probability of a Type I error, i.e., rejecting a null hypothesis that is true.
  
- Confidence level: The confidence level indicates the probability of obtaining the same results if you repeat the same data collection processes like tests, polls, or surveys.
  The confidence level is established before conducting the data integration and collection, typically between 90% and 99%. It helps you determine whether the results from sample data apply to the entire population.
  The confidence level and significance level are related. If you have a 95% confidence interval, it corresponds to a 5% significance level in the hypothesis test.
  Confidence level indicates the probability of drawing accurate conclusions based on sample data. The significance level is a concept that deals with testing a hypothesis and avoiding a type I error,
  	while the confidence level deals more with the precision of the results despite the repetition of the test. These two concepts have an inverse relationship, meaning that if the significance level increases,
  	the confidence level decreases, and vice versa. If you want to be more certain of the conclusions you draw, your confidence level needs to be higher.
  Both confidence level and significance level are used to determine the confidence interval.  Confidence level = 1-Significance level

![Function](https://github.com/amitmse/in_Python_/blob/master/Others/Significance%20Level%20Confidence%20Level.png)

- Confidence Intervals: It is a range of values within which the true population value likely falls, with a certain level of confidence.
  Think of it as an estimate, but instead of just giving a single number, it provides a range.
  This range is calculated from a sample of data and tells you how accurately your sample represents the entire population.
  Margin of Error: The width of the confidence interval is related to the margin of error, which indicates how much your sample estimate might vary from the true population value. 

- P-value: Probability of observing test results at least as extreme as the results observed, assuming the null hypothesis is correct. It helps determine the strength of the evidence against the null hypothesis.
  P value is compared with a significance level. It does not provide information about the magnitude of the effect. 
  The p-value is calculated based on the data and the test statistic (e.g., t-statistic, z-statistic) used to test the hypothesis.
  Once test statistic "t" calculted, find the associated p-value by referring to a t-distribution table.
  
  If the p-value ≤ α: Reject the null hypothesis, suggesting sufficient evidence in the data supports the alternative hypothesis.
  If the p-value > α: Do not reject the null hypothesis, suggesting insufficient evidence to support the alternative hypothesis.

- Critical region: All sets of values that lead to rejecting the null hypothesis lie in the critical region. Critical value separates the critical region from the non-critical region.
  Critical value is the value of the test statistic which defines the upper and lower bounds of a confidence interval.
  
- One-Tailed test Hypothesis Testing: Also called a directional test. critical distribution area is one-sided, meaning the test sample is either greater or lesser than a specific value.
  Two tails: the critical distribution area is two-sided.
  
- Statistical power refers to the probability that a statistical test will correctly reject a false null hypothesis.

- Limitations of Hypothesis Testing
  It cannot prove or establish the truth: Hypothesis testing provides evidence to support or reject a hypothesis, but it cannot confirm the absolute truth of the research question.
  Results are sample-specific: Hypothesis testing is based on analyzing a sample from a population, and the conclusions drawn are specific to that particular sample.
  Possible errors: During hypothesis testing, there is a chance of committing type I error (rejecting a true null hypothesis) or type II error (failing to reject a false null hypothesis).
  	Type 1 Error: A Type-I error occurs when sample results reject the null hypothesis despite being true.
  	Type 2 Error: A Type-II error occurs when the null hypothesis is not rejected when it is false, unlike a Type-I error.
  Assumptions and requirements: Different tests have specific assumptions and requirements that must be met to accurately interpret results.

- Degrees of Freedom refer to the number of independent pieces of information available when calculating a statistic.
  Trying to find the average of a group of numbers given total sum and the count of numbers are know.
  It means the last number is automatically determined for average computation, meaning it doesn't have freedom to vary independently.
  So, the degrees of freedom would be one less than the count of numbers, as freely choose all but the last one.

---------------------------------------------------------------------------------------------

# Probability Distribution:
- Probability distributions function (PDF) describe what we think the probability of each outcome is.
- probability mass function is used to describe the probabilities of discrete random variables,
  while the PDF is used to describe the probabilities of continuous random variables.
- They come in many shapes, but in only one size: probabilities in a distribution always add up to 1.
- A probability distribution is a function that assigns to each event a number in [0,1] which is 
  the probability that this event occurs.
- PDF shows the probability at a single point, and the cumulative distribution function (CDF) shows the cumulative probability up to that point.
- A statistical model is a set of probability distributions. We assume that the observations are generated 
  from one of these distributions.
- Why Probability Density and why not Probability: The concept of probability density is used for continuous random variables because in such cases, the probability of any specific value is infinitesimally small.
  This is because the number of possible values that a continuous random variable can take is infinite, making it impossible to assign a non-zero probability to any individual value.
  Instead, we use probability density to describe the distribution of continuous random variables.
- The probability distribution shows the probability of a point, and the CDF tells the probability of everything up to that point.  
- Chart: Horizontal axis set of possible numeric outcomes. Vertical axis probability of outcomes.
- Example:
	- Flipping a fair coin has two outcomes: it lands heads or tails. 
	- Before the flip, we believe there’s a 0.5 probability, of heads and same for tails. 
	- That’s a probability distribution over the two outcomes of the flip (Bernoulli distribution).
	
![Function](https://github.com/amitmse/in_Python_/blob/master/Others/distribution.png)

- Continuous Probability Distribution: Gaussian Normal, Standard Normal (Z- dist.), Student-t, Uniform, Log-Normal, Chi-Square.
- Discrete Probability Distribution: Bernoulli, Binomial, Negative Binomial, Geometric, Poisson, Uniform (defined both).

---------------------------------------------------------------------------------------------
## Uniform distribution:
- Many equally-likely outcomes (Bernoulli):the uniform distribution, characterized by its flat PDF. 
- It can be defined for any number of outcomes or even as a continuous distribution.
- Function
	- PDF		: 1/(b-a) 	       {-infinite <- (a,b) -> infinite}
	- Mean  	: (a+b)/2
	- Variance 	: (b-a)^2/12
Example: 
	- Imagine rolling a fair die. The outcomes 1 to 6 are equally likely.

---------------------------------------------------------------------------------------------
## Bernoulli distribution:
- Bernoulli distribution has only two possible outcomes i.e. success and failure in a single trial
- The Bernoulli PDF has two lines of equal height, representing the two equally-probable outcomes of 0 and 1 at either end.
- Bernoulli Distribution is a special case of Binomial Distribution with a single trial
- Function
	- PDF		: P^x*(1-P)^(1-x)       {x in 0 or 1}
	- Mean  	: P
	- Variance 	: P(1-P)
- Example: 
	- Flipping a fair coin
	- it’s going to rain tomorrow or not

---------------------------------------------------------------------------------------------
## Binomial distribution:
- The binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution.
- Function
	- PDF		: [n!/(n-x)!*x!] * [P^x*(Q)^(n-x)]	{! factorial}
	- Mean  	: nP
	- Variance 	: nPQ
- Example: 
	- Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows 
	  the binomial distribution. Each flip is a Bernoulli-distributed outcome. Converted to binomial 
	  distribution when counting the number of successes, where each flip is independent and has 
	  the same probability of success.
	
	- Imagine an urn with equal numbers of white and black balls. Draw a ball and note whether it is black, 
	  then put it back and Repeat this process. How many times black ball was drawn? 
	  This count also follows a binomial distribution.

-------------------------------------------------------------------------------------------------
## Logistic distribution:
- The logistic distribution is a continuous probability distribution similar in shape to the normal distribution but with heavier tails.
  Heavier tail is due to its mathematical structure. While logistic and normal distribution both are symmetric and unimodal.
  The logistic distribution's PDF decays more slowly in the tails compared to the normal distribution,
  meaning there's a higher probability of observing values further from the mean.
  This characteristic makes the logistic distribution more robust to outliers and extreme values.
  
- sigmoid function  / logistic function (Cumulative Distribution Function) =  1 / (1+exp^-y)
- Probability Density Function: exp^-y / (1 + exp^-y)^2
- Mean = 0
- Variance = Phi^2 / 3
  
- The logistic distribution is a probability distribution (PDF), and its cumulative distribution function (CDF) is the sigmoid function.
- It's used in various fields, including logistic regression, neural networks, and modeling growth patterns.
- The key feature is that its cumulative distribution function (CDF) is the logistic function,
  which is crucial in mapping real numbers to probabilities, especially in machine learning applications.
- The distribution is symmetrical around its mean. The mode and median of the logistic distribution are equal to its mean.
- Below chart is Probability Density Function. CDF is S shaped.

https://www.acsu.buffalo.edu/~adamcunn/probability/standardlogistic.html

![Function](https://github.com/amitmse/in_Python_/blob/master/Others/Logistic-%20Normal.png)

---------------------------------------------------------------------------------------------	
## Hyper-Geometric distribution:
- Example: 
	- This is the distribution of that same count if the balls were drawn without replacement instead. 
	  Undeniably it’s a cousin to the binomial distribution, but not the same, because the probability 
	  of success changes as balls are removed. 
	- If the number of balls is large relative to the number of draws, the distributions are similar
	  because the chance of success changes less with each draw.
	
---------------------------------------------------------------------------------------------	
## Poisson distribution:
- Simialr to the binomial distribution, the Poisson distribution is the distribution of a 
  count - the count of times something happened. 
- The Poisson distribution is when trying to count events over a time given the continuous rate of events occurring
- Poisson Distribution is a limiting case of binomial distribution.
  As n approaches ∞ and p approaches 0 with the product np held fixed, the Binomial (n, p) distribution approaches the Poisson distribution with expected value λ = np.
- Function								
	- PDF		: Expo(-Mean)*{(Mean^x)/x!} 		{x in o,1,2,3,4,5}
	- Mean  	: Mean
	- Variance 	: Mean
- Example:
	- Packets arrive at routers, or customers arrive at a store, or things wait in some kind of queue Count 
	  of customers calling a support hot-line each minute doesn't follow binomial/Bernoulli but Poisson.
	- The number of emergency calls recorded at a hospital in a day.
	- The number of thefts reported in an area on a day.
	- The number of customers arriving at a salon in an hour.
	- The number of suicides reported in a particular city.
	- The number of printing errors at each page of the book.
	- The number of incoming calls at a call center in a day.
	
---------------------------------------------------------------------------------------------	
## Geometric distribution:
- If the binomial distribution is “How many successes?” then the geometric distribution is
  “How many failures until a success?”
- Example:
	- From simple Bernoulli trials arises another distribution. How many times does a flipped coin 
	  come up tails before it first comes up heads? This count of tails follows a geometric distribution.

---------------------------------------------------------------------------------------------
## Negative Binomial distribution:
- It's a simple generalization. It’s the number of failures until r successes have occurred,not just 1.
- Example:
	- If we flip a coin a fixed number of times and count the number of times the coin turns out heads is a binomial distribution.
   		If we continue flipping the coin until it has turned a particular number of heads say the third head-on flipping 5 times, then this is a case of the negative binomial distribution.
   	- For a situation involving three glasses to be hit with 7 balls, the probability of hitting the third glass successfully with the seventh ball can be obtained with the help of negative binomial distribution.
	- In a class, if there is a rumor that there is a math test, and the fifth is the second person to believe the rumor, then the probability of this fifth person to be the second person to
   		believe the rumor can be computed using the negative binomial distribution.

---------------------------------------------------------------------------------------------  
## Exponential distribution:
- The exponential distribution is one of the widely used continuous distributions. 
- It is often used to model the time elapsed between events.
- The exponential distribution should come to mind when thinking of "time until event", maybe "time until failure".
- Exponential distribution is widely used for survival analysis. From the expected life of a machine to 
  the expected life of a human, exponential distribution successfully delivers the result.
- There is a strong relationship between the Poisson distribution and the Exponential distribution.
- Function								
	- PDF		: λe^(-λx)    		{x ≥ 0}
	- Mean  	: 1/λ
	- Variance 	: (1/λ)²
- Example: 
	- let’s say a Poisson distribution models the number of births in a given time period. 
	  The time in between each birth can be modeled with an exponential distribution.
	- Duration of a telephone call
	- How long does it take to perform a service, fix something at a service point etc.
	- Duration between two phone calls
	- Half life of atoms (radioactive decay)
	- Expected lifetime of electronic (or other) parts, 
	  if wearing is not considered (this is called Mean Time Between Failures, MTBF)
	- Age of plants or animals
	- Very simple model used by insurance companies

---------------------------------------------------------------------------------------------
## Weibull:
- Weibull distribution can model increasing (or decreasing) rates of failure over time. 
- The exponential is merely a special case.
- Commonly used to assess product reliability, analyze life data and model failure times
- Weibull isn’t an appropriate model for every situation i.e. chemical reactions and corrosion failures are 
  usually modeled with the lognormal distribution.

---------------------------------------------------------------------------------------------
## Normal Distribution / Gaussian distribution:
- The sum of Bernoulli trials follows a binomial distribution, and as the number of trials increases, 
  that binomial distribution becomes more like the normal distribution. 
- Its cousin the hyper-geometric distribution does too. 
- The Poisson distribution—an extreme form of binomial—also approaches the normal distribution as 
  the rate parameter increases.
- The mean, median and mode of the distribution coincide.
- The curve of the distribution is bell-shaped and symmetrical about the line x=μ.
- Normal distribution is another limiting form of binomial distribution.
- As n approaches ∞ while p remains fixed, the distribution of approaches the normal distribution with expected value 0 and variance 1.
  This result is sometimes loosely stated by saying that the distribution of X is asymptotically normal with expected value 0 and variance 1.
  This result is a specific case of the central limit theorem.
- Its popular due to Central Limit Theorem.
  The central limit theorem states that under certain (fairly common) conditions, the sum of many random variables will have an approximately normal distribution.
  The logarithm of various variables tend to have a normal distribution, that is, they tend to have a log-normal distribution 
- Function								
	- PDF		: [1/{SQRT(2Pai)*STD}]*Expo[(X-Mean)^2/-2Variance]   {-infinite <-x-> infinite}
	- Mean  	: Mean
	- Variance 	: Variance
- Example:
	- Heights of people, Measurement errors, Blood pressure, Points on a test, IQ scores, Salaries.


- In the Bayesian approach, all uncertainty is measured by probability.
  The Bayesian approach assumes the 'a priori' knowledge of probability models, in such a way that it is possible to build exact models of phenomena starting
  from experimental data, and then use the models to make predictions.
---------------------------------------------------------------------------------------------
### Standard Normal distribution: 
- It is also known as the Z distribution and it follows normal distribution with a mean of zero and a variance of one.
  The Standard Normal Distribution is used to standardize and compare different normal distributions by converting them into a single, common reference.
  This allows for easier calculation of probabilities and comparison of data points across various datasets.
  It also helps in hypothesis testing and identifying the probability of sample means significantly differing from population means.
  The standard normal distribution is a specific type of normal distribution with a mean of 0 and a standard deviation of 1. It is often used to calculate z-scores and probabilities.
  
  z = (x - μ) / σ  (x is raw value, μ mean and σ standard deviation)

  - Example: 
	- For example, if you get a score of 90 in Math and 95 in English, you might think that you are 
  	  better in English than in Math. However, in Math, your score is 2 standard deviations above 
	  the mean. In English, it’s only one standard deviation above the mean. It tells you that in Math, 
  	  your score is far higher than most of the students (your score falls into the tail).

---------------------------------------------------------------------------------------------
### Z-test:
- The sample is assumed to be normally distributed. A z-score is calculated with population parameters 
  such as "population mean" and "population standard deviation" and is used to validate a hypothesis 
  that the sample drawn belongs to the same population. Sample mean is same as the population mean.
- Function								
	- PDF		: (x - mean)/standard deviation
	- Mean  	: 0
	- Variance 	: 1			  

---------------------------------------------------------------------------------------------
## t /Student  Distribution:
- The t test tells how significant the differences between groups are. A t-test is used to compare the mean of 
  two given samples.
- A t-test is used when the population mean and population standard deviation are unknown.
- Independent samples t-test which compares mean for two groups. t test for equality of 
  population mean when variance is same.
- Before t test, F test is required for equality for variance.
- One sample t-test which tests the mean of a single group against a known mean.
- Test the significance of regression coefficient. 
- Example:
	- A very simple example: Let’s say you have a cold and you try a naturopathic remedy. 
	  Your cold lasts a couple of days. The next time you have a cold, you buy an over-the-counter 
	  pharmaceutical and the cold lasts a week. You survey your friends and they all tell you that 
	  their colds were of a shorter duration (an average of 3 days) when they took the homeopathic remedy. 
	  What you really want to know is, are these results repeatable? A t test can tell you by comparing
	  the means of the two groups and letting you know the probability of those results happening by chance.

	- Paired sample t-test which compares means from the same group at different times. 
	  Choose the paired t-test if you have two measurements on the same item, person or thing
		
---------------------------------------------------------------------------------------------		
## Chi-Squared Distribution :
- Tests for the strength of the association between two categorical variables. Chi Square lets you know whether 
  two groups have significantly different opinions, which makes it a very useful statistic for survey research.
- Population mean is known and test the variance of normal distributed. chi squared distribution is the square 
  of a normal distribution.
- Function
	- PDF		: (Observed - Mean)^2/Mean
	- Mean  	: mean
- Example:
	- The chi-squared distribution is used primarily in hypothesis testing
	- Goodness of fit test, which determines if a sample matches the population 
		(does a coin tossed 20 times turn up 10 heads and 10 tails?)
	- A chi-square fit test for two independent variables is used to compare two variables in a contingency table 
		to check if the data fits
	- Chi-squared test of independence in contingency tables (is there a relationship between gender and salary?)
	- Likelihood-ratio test for nested models
	- Log-rank test in survival analysis
	- Cochran–Mantel–Haenszel test for stratified contingency tables

---------------------------------------------------------------------------------------------
## Likelihood-ratio :
- This test assesses the goodness of fit of two competing statistical models based on the ratio of their likelihoods

---------------------------------------------------------------------------------------------
## F-test:
- F-test of equality of variances is a test for the null hypothesis that two normal populations have the same variance. 

- It is most often used when comparing statistical models that have been fitted to a data set, 
  in order to identify the model that best fits the population from which the data were sampled. 
  "F-tests" mainly arise when the models have been fitted to the data using least squares.


-----------------------------------------------------------------------------------------------------------------------------------

https://medium.com/@srowen/common-probability-distributions-347e6b945ce4

https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/

https://www.johndcook.com/blog/distribution_chart/

https://www.statisticshowto.datasciencecentral.com/probability-distribution/

-----------------------------------------------------------------------------------------------------------------------------------
## Covariance:
- It refers to the measure of how two variables will change (directional relationship) when they are compared to each other
- It measures the Variance between two variables
- Covariance indicates the direction of the linear relationship between variables. Correlation on the other hand measures 
   both the strength and direction of the linear relationship between two variables. 

## Correlation vs Regression:
- Correlation 
	- It measures the degree of relationship between two variables. 
	- correlation doesn’t capture causality.
	- Correlation between x and y is the same as the one between y and x.
- Regression analysis 
	- It is about how one variable affects another or what changes it triggers in the other.
	- Regression is based on causality (cause and effect).
	- Regression of x and y, and y and x, yields completely different results.
	
## ANOVA:
- Known as analysis of variance, is used to compare multiple (three or more) samples with a single test 
	i.e. all sample means are equal
	
------------------------------------------------------------------------------------------------------
# Ordinary Least Squares (OLS): 
- Finds parameter values that minimizing the error. 
## Assumptions of Linear regression: 6

### For Model:

	1. Linear in parameters : 
		Issue	: Beta not multiplied or divided by any other parameter. 
                          Incorrect and unreliable model which leads to error in result.
		Solution: Transformations of independent variables

### For Variable: 

	2. No perfect multicollinearity :
		Issue: Regression coefficient variance will increase.
  
		It inflates standard errors, leading to unstable and unreliable coefficient estimates, 
  		making it difficult to determine the individual effect of each predictor on the outcome and 
    		potentially leading to misleading conclusions about variable importance.
		
		The presence of multicollinearity makes the coefficient estimates highly sensitive to 
  		small changes in the data or model specification. Even a slight shift in the data can cause 
    		the coefficients to fluctuate significantly, making the model's interpretation unreliable.

		Multicollinearity does not directly affect the goodness-of-fit statistics of the model, 
  		such as R-squared or the overall F-test. 
    		The model can still make accurate predictions despite the presence of multicollinearity.

		Test: VIF 
  		VIF: [1.0 / (1.0 - R Squared)]
      		VIF = 1/T  (T refers to Tolerance = 1 – R² which is unexplained portion)
		R^2 is regressing each independent variable on the other independent variables.
  
  		Tolerance: It measures the influence of one independent variable on all other independent variables.
		It measures of how much a predictor variable's variance is not explained (1 – R²) by the other predictor variables 
  		in the model. It essentially assesses the degree to which a variable is independent of the others.
           
		Solution: Transformations of independent variables
		
### For Error Tearm: 4

	3. Normality of residuals : Differences between observed and predicted values
		In linear regression, the assumption that "errors follow a normal distribution" 
  		means the difference between the observed values and the values predicted by the model 
    		(the residuals) are assumed to be distributed according to a normal distribution. 
      		This assumption is crucial for the validity of statistical tests used in linear regression, 
		such as t-tests and ANOVA, which rely on the normality of the errors to provide accurate 
  		p-values and hypothesis tests for regression coefficients and confidence intervals.
     
		Issue: OLS estimators won’t have the desirable BLUE property. 
  		Standard errors and t-values used to calculate the statistical significance of regression 
  		parameters may be inaccurate, leading to potentially misleading conclusions.
    
  		Test: Jarque-Bera test, Kolmogorov-Smirnov Test, Shapiro-Wilk test, histograms or Q-Q plots
    
		Solution: Transforming the dependent or independent variables, Use robust statistical methods 
  				that are less sensitive to non-normality may be appropriate.
    
		
	4. Mean of residuals is zero :
		The sum of the residuals is always zero when an intercept is included in the regression model. 
  		This is because the regression line is fitted in a way that the total distance of the data points 
    		above the line is equal to the total distance of the data points below the line.
		If a model without an intercept is used, the residuals will not necessarily have a mean of zero.
		The mean of residuals is zero is a direct result of how the least squares method calculates 
  		the regression line. It's not an assumption that needs to be tested.
  
		Issue: Error terms has zero mean and doesn’t depend on the independent variables. 
			Thus, there must be no relationship between the independent variable and the error term.
			A model with a zero mean for residuals suggests that the model is, on average, 
   			neither overestimating nor underestimating the response variable
   
		Test: Plot of residuals against the fitted values. If the residuals are randomly scattered around zero, 
  			with no apparent trend, it suggests that the mean-zero assumption is met. 
     			A flat, horizontal line at zero in this plot would indicate a good fit.
			This can be done by calculating the mean of the residuals and comparing it to zero.
  
		Solution: The sum of residuals can always be zero; if they had some mean that differed from zero 
  			you could make it zero by adjusting the intercept by that amount. 
  
	
	5. Homoscedasticity of residuals /equal variance of residuals
		Homoscedasticity in the context of residuals refers to the assumption that the variance of 
  		the error terms (residuals) is constant across all levels of the independent variables 
    		in a regression model.

		Issue: Homoscedasticity is necessary for accurately estimating the standard errors of 
  			the regression coefficients. The standard errors of the coefficients may be biased, 
     			leading to unreliable hypothesis tests and confidence intervals.
			Homoscedasticity ensures that the estimated coefficients are unbiased and have minimum variance.
  
		Example	: Family income to predict luxury spending. Residuals are very small for low values of 
			  family income (less spend on luxury) while there is great variation in the size of 
			  the residuals for wealthier families. Standard errors are biased and it leads to 
			  incorrect conclusions about the significance of the regression coefficients
     
		Test	: Breush-Pagan test, Goldfeld-Quandt, Koenker-Bassett (generalized Breusch-Pagan)
			  Breush-Pagan: Calculate the Square the residuals, and Regress it on the independent variables.
  
		Solution: Weighted least squares regression.
			  Transform the dependent variable using one of the variance stabilizing transformations
	
	6. No autocorrelation (serial correlation) of residuals :
		Issue: correlation with own lag (stock price today linked with yesterday's price). if above fails 
		 	then OLS estimators are no longer the Best Linear Unbiased Estimators. While it does not 
			bias the OLS coefficient estimates, the standard errors tend to be underestimated 
			(t-scores overestimated) when the autocorrelations of the errors at low lags are positive.
			Low variance in unbiased estimator. Autocorrelation can lead to biased and unreliable 
			standard errors, affecting the validity of statistical tests and confidence intervals.
			Potentially missing key variables or an incorrect functional form. 
   
		Test :  Durbin-Watson Test: This test checks for autocorrelation of order one 
  			(correlation between consecutive residuals). This test ranges from 0 to 4.
			2: Indicates no autocorrelation. 
   			< 2: positive autocorrelation 
			> 2: Indicates negative autocorrelation.
   			Breusch-Godfrey Test: This test is designed to detect autocorrelation of any order in the residuals. 
			Autocorrelation Function (ACF) Plot: This plot displays the correlation of the residuals with 
   			their lagged values, helping to identify the lag order of autocorrelation
			Plotting the residuals over time can reveal patterns or trends, indicating autocorrelation. 

		Solution: Generalized Least Squares (GLS), 
  			Include lagged values of the dependent variable or independent variables in the regression model. 
			Transform the Data: Apply mathematical transformations to the data to reduce autocorrelation. 
			Use Time Series Models: If the data is time-series, consider using models specifically designed 
     			for time series analysis, such as ARIMA models      
	
	7. X variables and residuals are uncorrelated 
	
	8. Number of observations must be greater than number of Xs

#### Linear model should have residuals mean zero, have a constant variance, and not correlated with themselves or other variables. If these assumptions hold true, the OLS procedure creates the best possible estimates.

------------------------------------------------------------------------------------------------------

# Logistic regression 
- It uses MLE rather than OLS, it avoids many of the typical assumptions (listed below) tested in statistical analysis.
### Assumptions:
	1. Dependent variable should be binary
	2. Linearity between independent & log odds. (non-linear relationship between the dependent and independent variables)
	3. Independence of errors
	4. No perfect multicollinearity
### Does not assume: 
	- normality of variables (both DV and IVs)
	- linearity between DV and IVs
	- homoscedasticity
	- normal errors
	
# Model Metrics:
- beta(x) 	= covariance(x,y) / variance(x)
- correlation(x,y)= covariance(x,y) / [variance(x)*variance(y)]
- TSS 		= SUM[y-mean(y)]^2
- RSS 		= SUM[y-predicted(y)]^2
- R Squared	= 1.0 - (RSS/TSS)
- AIC		= (No of variable*2)               - (2*-Log Likelihood)
- BIC		= {No of variable*log(No of obs)}  - (2*-Log Likelihood)
- VIF 		= 1.0 / (1.0 - R Squared)
- Gini/Somer’s D = [2AUC-1] OR [(Concordant - Disconcordant) / Total  pairs]
- Divergence 	= [(meanG – meanB)^2] / [0.5(varG + varB)]       

			[meanG = mean of score only for good, varB= variance of score only for bad ]
			
- True Positives (TP) = Correctly Identified
- True Negatives (TN) = Correctly Rejected
- False Positives (FP) = Incorrectly Identified = Type I Error
- False Negatives (FN) = Incorrectly Rejected	= Type II Error
- Recall (Sensitivity) = Ability of the classifier to find positive samples from all positive samples
- Precision = Ability of the classifier not to label as positive a sample that is negative (positive predictive value)
- Specificity = Measures the proportion of actual negatives that are correctly identified (true negative rate)

![Function](https://github.com/amitmse/in_Python_/blob/master/Formula/Confusion%20Matrxi.jpg)

- True Positive Rate / Sensitivity / Recall : TP  / (TP + FN) = TP / Actual Positives
- True Negative Rate / Specificity : 	    TN  / (TN + FP) = TN / Actual Negatives
- False Positive Rate / Type I Error: 	    FP  / (FP + TN) = FP / Actual Negatives = 1 - Specificity
- False Negative Rate / Type II Error : 	    FN  / (FN + TP) = FN / Actual Positives = 1 - True Positive Rate
- Positive Predictive Value / Precision :     TP  / (TP + FP)
- Negative Predictive Value : 		    TN  / (TN + FN)
- False Discovery Rate: 			    FP  / (FP + TP) = 1 - Positive Predictive Value	
- Accuracy : 				   (TP + TN)/ (TP  + TN + FP + FN)
- F1-Score : 2*TP/ (2TP + FP + FN)   =   [2 * (Precision * Recall) / (Precision + Recall)]
	- F1 score (also F-score or F-measure) is a measure of a test's accuracy. 
	- The F1-score gives you the harmonic mean of precision and recall.
	- The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.
	- The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.
	- It considers both the precision and the recall of the test to compute the score: 
		- precision is the number of correct positive results divided by the number of all positive results returned by the classifier. precision is the measure of how accurate the classifier’s prediction of a specific class
		- recall is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). recall is the measure of the classifier’s ability to identify a class.
	 
			F1-Score : 2*TP	/ (2TP + FP + FN) = [2 * (Precision * Recall) / (Precision + Recall)]
	- If the classifier predicts the minority class but the prediction is erroneous and false-positive increases, the precision metric will be low and so as F1 score. Also, if the classifier identifies the minority class poorly, i.e. more of this class wrongfully predicted as the majority class then false negatives will increase, so recall and F1 score will low. F1 score only increases if both the number and quality of prediction improves. F1 score keeps the balance between precision and recall and improves the score only if the classifier identifies more of a certain class correctly.
	
- Precision/Specificity: how many selected instances are relevant.
- Recall/Sensitivity: how many relevant instances are selected.
- F1 score: harmonic mean of precision and recall.
- MCC: correlation coefficient between the observed and predicted binary classifications.
- AUC: relation between true-positive rate and false positive rate.
- Accuracy is not appropriate when the data is imbalanced. Because the model can achieve higher accuracy by just predicting accurately the majority class while performing poorly on the minority class which in most cases is the class we care about the most.

- Accuracy paradox: if the incidence of category A is dominant, being found in 99% of cases, then predicting that every case is category A will have an accuracy of 99%. Precision and recall are better measures in such cases. The underlying issue is that there is a class imbalance between the positive class and the negative class. Prior probabilities for these classes need to be accounted for in error analysis. Precision and recall help, but precision too can be biased by very unbalanced class priors in the test sets.

- Area under curve /C statistics = Percent Concordant + 0.5 * Percent Tied ( https://www.geeksforgeeks.org/how-to-handle-imbalanced-classes-in-machine-learning/ )
	The ROC curve is a graphical plot that illustrates the performance of any binary classifier 
	system as its discrimination threshold is varied. True positive rate (Sensitivity : Y axis ) 
	is plotted in function of the false positive rate (100-Specificity : X axis) for different 
	cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair 
	corresponding to a particular decision threshold.
- Standard Error Coef: 
	Linear regression standard error of Coef : SE  = sqrt [ S(yi - yi)2 / (n - 2) ] / sqrt [ S(xi - x)2 ]
	The standard error of the coefficient estimates the variability between coefficient estimates 
	that you would obtain if you took samples from the same population again and again. 
	The calculation assumes that the sample size and the coefficients to estimate would remain 
	the same if you sampled again and again. Use the standard error of the coefficient to measure 
	the precision of the estimate of the coefficient. 
	The smaller the standard error, the more precise the estimate.
- Recall and Precision:
	Recall is the fraction of instances that have been classified as true. On the contrary, 
	precision is a measure of weighing instances that are actually true. 
	While recall is an approximation, precision is a true value that represents factual knowledge.
- ROC curve:
	Receiver Operating Characteristic is a measurement of the True Positive Rate (TPR) against False 
	Positive Rate (FPR). We calculate True Positive (TP) as TPR = TP/ (TP + FN). On the contrary, 
	false positive rate is determined as FPR = FP/FP+TN where where TP = true positive, TN = true negative, 
	FP = false positive, FN = false negative.
- AUC vs ROC:
	AUC curve is a measurement of precision against the recall. Precision = TP/(TP + FP) and TP/(TP + FN).
	This is in contrast with ROC that measures and plots True Positive against False positive rate.

- Feature importances: 
	It is also known as the Gini importance. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  	That reduction or weighted information gain is defined as. The weighted impurity decrease equation is the following: 
	
		N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)
	
			N 	: Total number of samples
			N_t 	: No. of samples at the current node
			N_t_L 	: No. of samples in the left child 
			N_t_R 	: No. of samples in the right child
------------------------------------------------------------------------------------------------------
# Decision Tree 
## Gini Index:
Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.
- It works with categorical target variable “Success” or “Failure”.
- It performs only Binary splits
- Higher the value of Gini higher the homogeneity.
- CART (Classification and Regression Tree) uses Gini method to create binary splits.
### Steps to Calculate Gini for a split: 	
1. Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).
2. Calculate Gini for split using weighted Gini score of each node of that split
3. Example:
	- Total Student = 30 and only 15 play cricket(50%)
	- Split on Gender:- Female, Male
		1. Female = 10 (2  play cricket, Play= 2/10 = 0.2,   No Play = 8/10 =0.8 )
		2. Male	 = 20 (13 play cricket, Play= 13/20= 0.65,  No Play = 7/20 =0.35)
		3. Gini for sub-node Female = (0.2  )*(0.2 )+(0.8 )*(0.8 )=0.68 (p^2+q^2)
		4. Gini for sub-node Male = (0.65 )*(0.65)+(0.35)*(0.35)=0.55 (p^2+q^2)
		5. weighted Gini for Split Gender  = (10/30)*0.68+(20/30)*0.55   =0.59

	- Similar for Split on Class:- XII, X
		1. XII = 14  (6  play cricket, Play= 6/14 = 0.43,   No Play = 8/14 =0.57 )
		2. X   = 16  (9  play cricket, Play= 9/16 = 0.56,   No Play = 7/16 =0.44 )
		3. Gini for sub-node Class IX = (0.43 )*(0.43)+(0.57 )*(0.57)=0.51 (p^2+q^2)
		4. Gini for sub-node Class X = (0.56 )*(0.56)+(0.44 )*(0.44)=0.51 (p^2+q^2)
		5. Calculate weighted Gini for Split Class = (14/30)* 0.51 +(16/30)* 0.51 =0.51

	- Above, we can see that Gini score for Split on Gender is higher (0.59> 0.51) than Class so node will split on Gender

4. Above is Gini and below is for Gini Index (1-gini)

	https://github.com/amitmse/in_Python_/tree/master/Random_Forest
	
	- Gini Index:
		- for each branch in split:
			- Calculate percent branch represents #Used for weighting
			- for each class in branch:
		    		- Calculate probability of class in the given branch.
		    		- Square the class probability.
			- Sum the squared class probabilities.
			- Subtract the sum from 1. #This is the Ginin Index for branch
	    	- Weight each branch based on the baseline probability.
	    	- Sum the weighted gini index for each split.
------------------------------------------------------------------------------------------------------
## Chi-Square:
- It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measures it by sum of squares of standardized differences between observed and expected frequencies of target variable.
- It works with categorical target variable “Success” or “Failure”.
- It can performs two or more splits
- Higher the value of Chi-Square higher the statistical significance of differences between sub-node and Parent node.
- Chi-Square of each node is calculated using formula,
- Chi-square = SQRT((Actual – Expected)^2 / Expected)
- It generates tree called CHAID (Chi-square Automatic Interaction Detector)
	- Steps to Calculate Chi-square for a split:
		- Calculate Chi-square for individual node by calculating the deviation for Success and Failure both
		- Calculated Chi-square of Split using Sum of all Chi-square of success and Failure of each node of the split					
		
- Example: 
	- Let’s work with above example that we have used to calculate Gini.
	- Split on Gender:
		1. First we are populating for node Female, Populate the actual value for “Play Cricket” and “Not Play Cricket”, here these are 2 and 8 respectively.
		2. Calculate expected value for “Play Cricket” and “Not Play Cricket”, here it would be 5 for both because parent node has probability of 50% and we have applied same probability on Female count(10).
		3. Calculate deviations by using formula, Actual – Expected. It is for “Play Cricket” (2 – 5 = -3) and for “Not play cricket” ( 8 – 5 = 3).
		4. Calculate Chi-square of node for “Play Cricket” and “Not Play Cricket” using formula with formula, = ((Actual – Expected)^2 / Expected)^1/2.
		5. Follow similar steps for calculating Chi-square value for Male node.			
		6. Now add all Chi-square values to calculate Chi-square for split Gender.
			- Play Cricket	 			PC
			- Play not Cricket 			NPC 		
			- Expected 	Play Cricket 		EPC	[Total*%oftarget ]
			- Expected 	Play not Cricket	ENPC	[Total*%oftarget ]
			- Deviatation 	Play Cricket		DPC	[PC - EPC]
			- Deviatation 	NOT Play Cricket	DNPC	[NPC - ENPC]
			- Chi-Square  	Play Cricket		CPC 	[(DPC^2)/EPC]^1/2
			- Chi-Square  	Not Play Cricket	CNPC	[(DNPC^2)/ENPC]^1/2


		7. TOTAL CHI-SQUARE for Gender = 1.34 + 1.34 + 0.95 + 0.95 = 4.58
		8. TOTAL CHI-SQUARE for Class  = 0.38 + 0.38 + 0.35 + 0.35 = 1.46

## Information Gain
- We can say that less impure node requires less information to describe it and more impure node requires more information. Information theory has a measure to define this degree of disorganization in a system, which is called Entropy. Lower Entropy is better. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided it has entropy of one. 
- Entropy can be calculated using formula:   - P*Log2(P) - Q*Log2(Q)
	- Here P and Q is probability of success and failure respectively in that node. 
	- Entropy is also used with categorical target variable. 
	- It chooses the split which has lowest entropy compared to parent node and other splits.			
- Steps to calculate entropy for a split:
	- Calculate entropy of parent node
	- Calculate entropy of each individual node of split and calculate weighted average of all sub-nodes available in split.			
- Example: 
	- Let’s use this method to identify best split for student example. 	Total=30, PC=15, NPC=15, P=PC/Total, Q=NPC/Total			
	- Entropy for parent node = -(15/30)log2(15/30) – (15/30)log2(15/30) 	= 1. 	Here 1 shows that it is a impure node. 
	- Entropy for Female node = -(2/10)log2(2/10) – (8/10)log2(8/10) 	= 0.72 and for male node   = -(13/20)log2(13/20) – (7/20)log2(7/20) 	= 0.93.
	- Entropy for split Gender = Weighted entropy of sub-nodes [10 Female, 20 Male] = (10/30)*0.72 + (20/30)*0.93 					= 0.86 
	- Entropy for Class IX node = -(6/14) log2 (6/14) – (8/14) log2 (8/14) 	= 0.99 and for Class X node  = -(9/16) log2 (9/16) – (7/16) log2 (7/16) = 0.99
	- Entropy for split Class =  (14/30)*0.99 + (16/30)*0.99 		= 0.99
	- Above you can see that entropy of split on Gender is lower compare to Class so we will again go with split Gender. 
	- We can derive information gain from entropy as 1- Entropy.

- Entropy:
    	- for each branch in split:
		- Calculate percent branch represents #Used for weighting
			- for each class in branch:
	    			- Calculate probability of class in the given branch.
	    			- Multiply probability times log(Probability,base=2)
	    			- Multiply that product by -1
			- Sum the calculated probabilities.
    	- Weight each branch based on the baseline probability.
    	- Sum the weighted entropy for each split.


## Reduction in Variance
- Till now, we have discussed the algorithms for categorical target variable. Reduction in Variance is an algorithm for continuous target variable. This algorithm uses the same formula of variance to choose the right split that we went through the descriptive statistics. 
- The split with lower variance is selected as the criteria to split the population:
- Steps to calculate Variance:
	- Calculate variance for each node.
	- Calculate Variance for each split as weighted average of each node variance
			
- Example: 
	- Let’s assign numerical value 1 for play cricket and 0 for not playing cricket. 
	- Now follow the steps to identify the right split:
	- Variance for Root node, here mean value is (15*1 + 15*0)/30 = 0.5 and we have 15 one and 15 zero. 
	
		Now variance would be ((1-0.5)^2+(1-0.5)^2+….15 times+(0-0.5)^2+(0-0.5)^2+…15 times) / 30,
		
		this can be written as (15*(1-0.5)^2+15*(0-0.5)^2) / 30 = 0.25
	- Mean of Female node =(2*1+8*0)/10=0.2 and Variance = (2*(1-0.2)^2+8*(0-0.2)^2) / 10 = 0.16
	- Mean of Male Node =(13*1+7*0)/20=0.65 and Variance = (13*(1-0.65)^2+7*(0-0.65)^2) / 20 = 0.23
	- Variance for Split Gender = Weighted Variance of Sub-nodes = (10/30)*0.16 + (20/30) *0.23 = 0.21
	- Mean of Class IX node =(6*1+8*0)/14=0.43 and Variance = (6*(1-0.43)^2+8*(0-0.43)^2) / 14 = 0.24
	- Mean of Class X node =(9*1+7*0)/16=0.56 and Variance = (9*(1-0.56)^2+7*(0-0.56)^2) / 16 = 0.25
	- Variance for Split Gender =Weighted Variance of Sub-nodes = (14/30)*0.24 + (16/30) *0.25 = 0.25
Above, you can see that Gender split has lower variance compare to parent node so 
the split would be on Gender only.
------------------------------------------------------------------------------------------------------
# Random Forest
## Bootstrap samples:
- Draw repeated samples from the population, a large number of times. 
- Samples are approximatively independent and identically distributed (i.i.d.).

## Ensemble methods:
- Ensemble learning is a machine learning paradigm where multiple models (often called "weak learners") are trained to solve the same problem and combined to get better results. 
	
## Bagging (Bootstrap aggregating):
- Fit a weak learner (several independent models) on each of bootstarp samples and finally aggregate the outputs (average model predictions) in order to obtain a model with a lower variance. It builds model parallelly.

## Boosting:
- Similar to bagging but it fits weak learner sequentially (a model depends on the previous ones) in a very adaptative way. Each model in the sequence is fitted giving more importance to the observations which are not classified correctly (high error). Mainly focus on reducing bias.
	
- Bagging mainly focus at getting an ensemble model with less variance than its components whereas boosting and stacking will mainly try to produce strong models less biased than their components (even if variance can also be reduced).

## Stacking:
- Stacking mainly differ from bagging and boosting on two points. First stacking often considers heterogeneous weak learners (different learning algorithms are combined) whereas bagging and boosting consider mainly homogeneous weak learners. Second, stacking learns to combine the base models using a meta-model whereas bagging and boosting combine weak learners following deterministic algorithms.
------------------------------------------------------------------------------------------------------	
## Bias - Variance
### Bias: 
- Bias is the difference between the prediction of model and actual value. 
- It always leads to high error on training and test data.
- It creates underfitting problem.
- If model is too simple and has very few parameters then it may have high bias and low variance.

### Variance: 
- Model performs very well on development data but poor performance on on OOT validation.
- It creates overfitting problem.
- If model has large number of parameters then it’s going to have high variance and low bias
- For high variance, one common solution is to reduce parameter/features. 
- This very frequently increases bias, so there’s a tradeoff to take into consideration.

## Mean Decrease in Accuracy (MDA) / Accuracy-based importance / Permutation Importance:
- The values of the variable in the out-of-bag-sample are randomly shuffled, keeping all other variables the same. Finally, the decrease in prediction accuracy on the shuffled data is measured. 
- The mean decrease in accuracy across all trees is reported. 
- For example, age is important for predicting that a person earns over $50,000, but not important for predicting a person earns less. Intuitively, the random shuffling means that, on average, the shuffled variable has no predictive power. This importance is a measure of by how much removing a variable decreases accuracy, and vice versa — by how much including a variable increases accuracy.

- Note that if a variable has very little predictive power, shuffling may lead to a slight increase in accuracy due to random noise. This in turn can give rise to small negative importance scores, which can be essentially regarded as equivalent to zero importance.	
		
- This is most interesting measure, because it is based on experiments on out-of-bag(OOB) samples, via destroying the predictive power of a feature without changing its marginal distribution.
	
- Percentage increase in mean square error is analogous to accuracy-based importance, and is calculated by shuffling the values of the out-of-bag samples.

## Gini Importance / Mean Decrease in Impurity (MDI) :
- Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset. It’s calculated as 
- Gini impurity index (G) = P * (1 - P)
- Importance = G (parent node) - G (child node 1) - G (child node 2)
- The initial gini index before split  Overall = 1 − P(success)^2 − P(Failure)^2
- Node level :
	- impurity in Left node  =1 − P(Success in left node)^2  − P(Failure in left node)^2
	- impurity in Right node =1 − P(Success in right node)^2 − P(Failure in right node)^2
- Now the final formula for GiniGain would be = Overall − impurity in  Left node − impurity in Right node
	- Lets assume we have 3 classes and 80 objects. 19 objects are in class 1, 21 objects in class 2, and 40 objects in class 3 (denoted as (19,21,40) ). 
	- The Gini index would be: 	= 1 - [ (19/80)^2 + (21/80)^2 + (40/80)^2] = 0.6247      
		- costbefore Gini(19,21,40) = 0.6247

	- In order to decide where to split, we test all possible splits. For example splitting at 2.0623, 
		- which results in a split (16,9,0) and (3,12,40).
		- After testing x1 < 2.0623:
			- costL Gini(16,9,0)  = 0.4608
			- costR Gini(3,12,40) = 0.4205
	- Then we weight branch impurity by empirical branch probabilities: costx1<2.0623 = 25/80 costL + 55/80 costR = 0.4331
	- We do that for every possible split, for example x1 < 1:
		- costx1<1 = FractionL Gini(8,4,0) + FractionR Gini(11,17,40) = 12/80 * 0.4444 + 68/80 * 0.5653 = 0.5417
	- After that, we chose the split with the lowest cost. This is the split x1 < 2.0623 with a cost of 0.4331.

-----------------------------------------------------------------------------------------------------------------------

## Model Validation:

	- Any good model always has weakness
	- Where does this model fail?
	- What are its hidden weaknesses?
	- Under what conditions does it break down?
	- How can it be exploited or misused?
 	- Find a blind spots before they become costly failures

	-------------------------------------------------------------------------------------------------------------------

 	Regulators expect ML models to comply with the standards of SR 11-7 part of model risk management (MRM).
	
	- Conceptual Soundness: Assess the quality of the model design and construction, limitations,
		reviewing the model documentation, assessing empirical evidence, and confirming that 
		the variable selection process used in the model is conceptually sound.
		Decisions based on incorrect or misused model outputs and reports.
    
	- Data Integrity/Representativeness: the data used for model development be representative 
		of the bank’s portfolio and market/business conditions
  
	- Out of time validation. Also k-fold cross-validation, Stratified K-Fold, time-based splits can be used.
		Validation guides model refinement during development, and testing validates its performance 
  		in real-world contexts, ensuring it behaves reliably and effectively beyond the training data.

	- Check Bias and Variance:
		Check bias in human decision-making is carried over to the development.
		The data-generating process itself can be biased.
		One way to identify data bias is by benchmarking with other models.
		Random selection of development sample.
 
 		Assess model's bias (error due to assumptions).
  		Assess model's Variance (sensitivity to training data fluctuations).
		To check above perform: 
  
		- Cross-Validation: It reveals model's performance is consistent across different data samples, 
			indicating lower variance. 
			It also helps identify if the model's assumptions are too restrictive, leading to high bias.
  
		- Learning Curves: Plot the training and validation errors against the size of the training dataset.
  			Interpreting the curves:
			High Bias (Underfitting): If both training and validation errors are high and close together, 
			the model may be too simple and not capturing the underlying patterns in the data.
			High Training Error, High Validation Error: Suggests underfitting (high bias).
       
			High Variance (Overfitting): If the training error is low but the validation error is high, 
			the model is overfitting the training data and not generalizing well to unseen data
			Low Training Error, High Validation Error: Indicates overfitting (high variance).
    
		- Sensitivity Analysis: Test the model's sensitivity to changes in input variables.
 
	- Check model metrics: accuracy, precision, recall, F1-Score, and error rates (MAE, RMSE).
		Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values.
		Mean Absolute Error (MAE): Measures the average absolute difference between predicted and actual values.
		Root Mean Squared Error (RMSE): It helps interpret errors in the same units as the target variable.
			It follows an assumption that errors are unbiased and follow a normal distribution. 
      
		Bias towards the Majority Class, Actual Performance of the Minority Class. Below metrics help to indentify:
		F1-Score: Provides a harmonic mean of precision and recall, accounting for both metrics and balancing 
			their importance. Why harmonic mean and not an arithmetic mean: HM punishes extreme values.
	 
		G-Mean: Computes the geometric mean of sensitivities for each class, providing a better overall picture 
			of performance across all classes.
	 
		----------------------------------------------------------------------------------------------------
		Gini coefficient: It's ratio between the area between the ROC curve and the diagonal line & 
  			the area of the above triangle. 
  			Gini = 2*AUC – 1

		Gain and Lift charts: Check the rank ordering of the probabilities.
  			This graph tells how well is model is segregating positive from negative.
			Lift is dependent on the total response rate of the population. Hence, if the response rate 
   			of the population changes, the same model will give a different lift chart.
     
		Kolomogorov Smirnov Chart: KS measures the degree of separation between the positive and negative.
  		----------------------------------------------------------------------------------------------------
    		AUC-ROC: Model's ability to distinguish between classes.
			The biggest advantage of using the ROC curve is that it is independent of the change 
   			in the proportion of positive class.
			It considers the predicted probabilities for determining our model’s performance. 
   			Issue: it only takes into account the order of probabilities, and does not take into account 
      			the model’s capability to predict a higher probability for samples more likely to be positive.
			The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity).
  		----------------------------------------------------------------------------------------------------
		Accuracy: Measures the overall proportion of correct predictions. 
		Precision: Measures the proportion of true positives among all positive predictions.
		Recall (Sensitivity): Measures the proportion of actual positives that are correctly identified.
    		Precision-Recall Curves: Visualize the trade-off between precision and recall across different thresholds, 
      			enabling a deeper understanding of the model's performance under various scenarios.
	 
  		Log Loss: Measures the performance of a classification model based on probability predictions.
    			It's a negative average of the log of corrected predicted probabilities.
       
		R-squared (Coefficient of Determination): Measures the proportion of variance in the dependent variable 
  			that can be predicted from the independent variables.
  		----------------------------------------------------------------------------------------------------
    
	- Model Interpretability and Explainability in Validation:
		Interpretability: It helps identify potential model weaknesses, fostering robustness and reliability.
		credit-scoring model relies too heavily on a single variable, leading to biased decisions.
   		
		Explainability: It builds trust by shedding light on the factors driving the model's decisions.
		why certain financial behaviors contribute more to the model's risk assessment.
  
		To provide explanations for complex models like neural networks, RF and GBM use methods like:
  		------------------------------------------------------------------------------------------------------
  		LIME: Local Interpretable Model-Agnostic Explanations. 
    		It creates a simplified, interpretable model to explain the predictions of a complex model.
    		It provides Local Explanations for individual predictions, focusing on how each input feature 
      		contributes to that specific prediction.
		-------------------------------------------------------------------------------------------------------
		SHAP: SHapley Additive exPlanations. It explain the output of machine learning models by assigning 
		importance values to each feature based on their contribution to the prediction. 
    		It's a game-theoretic approach that calculates the average marginal contribution of each feature, 
      		helping to understand how each feature affects the model's output
      		SHAP to provide overall (Global) explanations of how the model as a whole makes predictions, 
		focusing on the overall influence of each feature.
      		--------------------------------------------------------------------------------------------------------
		Feature Importance: Analyze the importance of each input feature in the model's predictions. 
		Techniques like tree-based models or methods that calculate the importance of each feature based 
  		on its contribution to the model's predictions.
		Tree-Based Algorithms feature importance scores based on how much each feature reduces impurity 
  		(e.g., Gini index or information gain) in the decision tree nodes.
    		Tree based: Decision Trees, Random Forests, XGBoost, LightGBM.
		Linear Models (Logistic Regression, Linear Regression) feature importance is derived from the coefficients 
  		of the linear model. Features with larger absolute coefficient values are considered more important. 
		Neural Networks Techniques like SHAP (SHapley Additive exPlanations) values or feature importance 
  		via gradient analysis is used to understand the contribution of individual features.
		Permutation Feature Importance: This technique measures the contribution of a feature by measuring 
  		the changes in the model performance after randomly shuffling its values. 
		------------------------------------------------------------------------------------------------------------

	- Vendor-model: outcomes analysis, sensitivity analysis, benchmarking, monitoring.

	- Hyperparameters: Validation is crucial for selecting the best model, tuning hyperparameters, 
 		and ensuring the model can adapt to new situations. 
		It is the process of finding the best set of hyperparameters for a model to maximize its performance.
  
		------------------------------------------------------------------------------------------------------------    
		Random Forest hyperparameters:
  			- Number of trees: The number of decision trees in the forest. Generally, 
     				a larger number of trees can improve accuracy but also increase training time.
  			- Maximum depth of each tree: A deeper tree can capture more complex relationships in the data 
     				but may also lead to overfitting.
			- Minimum number of samples required to split a node:  Helps prevent overfitting by ensuring 
       				that nodes aren't split on very small subsets of the data.
			- Minimum number of samples per leaf node: The minimum number of samples required to be 
   				at a leaf node. It helps to prevent overfitting by ensuring that leaf nodes have a 
       				sufficient number of samples.
			- Number of features to consider when making a split: IT controls the diversity of the trees 
   				in the forest, with more features leading to potentially more diverse trees.
			- Bootstrap: Determines whether or not to use bootstrap sampling when building the trees. 
   				Bootstrap sampling involves drawing samples with replacement, which can help increase diversity. 
			- Criterion: The function used to measure the quality of a split. 
   				Common choices include "gini" for Gini impurity and "entropy" for information gain. 
			- Class weight: To adjust the weights of classes in imbalanced datasets, which can be useful 
   				when one class is significantly more prevalent than others.
       
		------------------------------------------------------------------------------------------------------------
		AdaBoost hyperparameters:
  			- Number of Estimators: This determines how many weak learners (e.g., decision trees) are 
			combined in the ensemble. More estimators can improve accuracy but also increase training time. 
			- Learning Rate: This controls the contribution of each weak learner to the final prediction. 
				A smaller learning rate means each weak learner has less influence, potentially requiring 
				more estimators to achieve the same performance. 
			- Base Estimator Hyperparameters: If the base estimator (e.g., decision trees) has its own hyperparameters 
			(like max_depth for decision trees), tuning these can also impact the AdaBoost model's performance. 
			- Loss Function (loss): AdaBoost supports different loss functions for classification, like exponential, 
			linear, and square, which affect how weights are assigned to misclassified samples. 
			- Random Seed: Setting a random seed ensures reproducibility, but experimenting with different random seeds 
			during hyperparameter tuning can improve the robustness of the model.
   
		------------------------------------------------------------------------------------------------------------
		Gradient Boosting hyperparameters:
			- Learning Rate: This controls the contribution of each tree to the final prediction. 
   				A smaller learning rate leads to more stable and robust models, but requires more trees 
       				to achieve optimal performance. 
			- Number of Estimators (Trees): This parameter dictates how many trees are used in the ensemble. 
   				A larger number of trees can improve performance, but also increases computational cost 
       				and risk of overfitting. 
			- Max Depth: This limits the complexity of individual trees, preventing overfitting by restricting 
   				how deep they can grow. 
			- Subsampling: This involves randomly selecting a subset of the training data for each tree. 
   				Subsampling helps to prevent overfitting and can improve the generalizability of the model, 
       				explains a guide on Hands-On Machine Learning with R. 
	   
		------------------------------------------------------------------------------------------------------------
   		XGBoost hyperparameters: 
  			- Maximum depth of each tree: A deeper tree can capture more complex relationships in the data 
     				but may also lead to overfitting.
			- minimum sum of instance weights (Hessian) needed in a child. It helps prevent overfitting 
   				by controlling the creation of new nodes in the tree. 
			- subsample: This determines the fraction of training instances used for each tree, 
   				reducing the risk of overfitting. 
			- colsample bytree: This parameter specifies the fraction of features used for each tree. 
   				Similar to subsample, it helps prevent overfitting by reducing the model's 
       				reliance on specific features. 
			- learning rate (eta): This parameter controls the step size of the gradient descent algorithm. 
   				A smaller learning rate can lead to more stable training 
       				but may require more iterations to converge. 
			- gamma: This parameter specifies the minimum loss reduction required to make a split. 
   				It can be useful for pruning the tree and preventing overfitting. 
			- L2 regularization: This parameter adds a penalty proportional to the squared magnitude 
   				of the coefficients, helping to prevent overfitting. 
			- L1 regularization: This parameter adds a penalty proportional to the absolute value of 
   				the coefficients, promoting sparsity in the model. 
       
		------------------------------------------------------------------------------------------------------------
		In neural networks, hyperparameters are settings that are not learned during training but are 
  		set beforehand and influence the model's architecture, learning process, and overall performance.
		- Number of Layers: The number of hidden layers significantly impacts the model's complexity and 
  			ability to learn intricate patterns. 
		- Number of Neurons per Layer: The width of each hidden layer influences the model's capacity to represent 
  			complex relationships. 
		- Learning Rate: This determines the step size during optimization, affecting how quickly the model converges 
  			to the minimum loss.
		- Batch Size: The number of training examples processed before updating model parameters. 
  			Larger batch sizes can lead to faster convergence but might require more memory. 
		- Optimizer: The algorithm used to update model weights (e.g., Adam, SGD) influences the speed and 
  			stability of training. 
		- Epochs: The number of times the entire training dataset is passed through the model.
		- Activation Function: The activation function introduced nonlinearity into the model, 
  			enabling it to learn complex relationships.
		- Regularization Techniques: These help prevent overfitting by adding penalties to the model's complexity 
  			(e.g., L1, L2 regularization).
		- Dropout Rate: Randomly drops out neurons during training, preventing over-reliance on specific neurons 
  			and improving generalization.

		------------------------------------------------------------------------------------------------------------
		Use techniques like Grid Search, Randomized Search or Bayesian Optimization to explore 
  		the parameter space and find the optimal combination.
		- Grid Search: Defines a grid of hyperparameter values and tests all possible combinations. 
			Pro: Simple to understand and implement. Finding the best combination within the defined grid.
			Cons: Computationally expensive, especially with many hyperparameters or large grids.
  				May miss the optimal hyperparameters if the grid is not fine-grained enough.
		- Random Search: Randomly samples hyperparameters from a defined range or distribution, 
  			without evaluating all combinations.
			Pros: More efficient than grid search, especially with high-dimensional hyperparameter spaces.
				Can find good hyperparameters with fewer evaluations. 
			Cons: May not find the optimal hyperparameters, especially if the search space is large.
				Requires careful selection of the number of iterations to balance exploration and efficiency. 
		- Bayesian Optimization: Uses a probabilistic model (typically a Gaussian process) to learn the relationship 
  			between hyperparameters and performance. It then intelligently explores the hyperparameter space, 
     			focusing on regions with the highest probability of containing the optimal hyperparameters.
			Pros: Efficient than grid and random search, requirs fewer evaluations to find good hyperparameters.
				Can handle non-convex and noisy objective functions. 
    				Can be parallelized to speed up the search process. 
			Cons: Can be computationally expensive for each iteration, especially with complex models.
				Requires a suitable Gaussian process model and careful selection of hyperparameters for the model.




------------------------------------------------------------------------------------------------------------------------

## ML Model Issue:

	1. A high-performing model can still be completely wrong: Instead of a logical monotonic relationship 
 		(higher credit score → lower default risk), the model learned a non-monotonic pattern.

	2. Hidden Model Weaknesses Lurking Beyond Aggregate Metrics: Failure clustering analysis uncovered a 
 		major issue—model performance is not homogeneous across input segments. Some clusters exhibit 
   		large mean absolute residuals (especially cluster 0) meaning our model struggles significantly 
     		in certain regions of the input space.

	3. Harmful Side Effects of Variables: credit score is not only influencing model predictions significantly 
 		but is also a primary driver of model errors (particularly, for medium and low scores). 
		Its interaction with credit utilization further amplifies these errors, as shown clearly 
  		by the main effect and interaction plots. 
    		When a variable strongly impacts both model outputs and errors, it's a flashing red flag that 
      		your model might be misaligned, unstable, or missing critical interactions. 
		Ignoring this could mean you're building on shaky ground.

	4. Segment-level Miscalibration: Machine learning models, like XGBoost, often struggle with probability 
 		miscalibration—particularly within specific segments of your data. 
		Proper calibration—using techniques like Platt Scaling, Isotonic Regression, or Venn-Abers 
  		Prediction—is not merely beneficial; 
		it's essential for critical tasks such as credit default prediction, where accurate probabilities 
  		directly influence business decisions.

	5. Performance Fragility--Weak Resilience against Distribution Drift:
		Performance fragility due to distribution drift can significantly impact the effectiveness of models 
  		in production environments. 
		Distribution drift occurs when the statistical properties of the input data change over time
		This fragility manifests as deteriorating prediction performance when model performance is 
  		not homogeneous across input clusters. 
		From the observations related to Holes #2, #3, and #4, we recognize that model performance 
  		is not uniform across different data segments. 
		Such non-homogeneity translates into fragility during distribution drift in production.
		To anticipate and mitigate the risk associated with distribution drift, 
  		it is essential to simulate various drift scenarios (https://lnkd.in/eAfngeAA). 
		Model resilience testing involves creating synthetic drift conditions that the model may encounter 
  		in real-world applications as shown in the Figures below. 
		Measuring feature distribution drift is pivotal in understanding how drift impacts model performance. 
		Various metrics can be employed to quantify feature drift and assess model stability.
		
  		Kolmogorov-Smirnov (KS):The KS test compares the cumulative distributions of two datasets 
    			and identifies any significant differences. 
       			It helps quantify how the distribution of features has altered over time.
       
		Wasserstein Distance: The Wasserstein Distance, also known as Earth Mover's Distance, measures the minimum 
  			amount of work required to transform one distribution into another. 
     			It provides a clear indication of feature distribution shifts.
	
		Jensen-Shannon Distance (Population Stability Index, PSI): The Jensen-Shannon Distance is a symmetric measure 
  			of divergence between two probability distributions. 
			The Population Stability Index (PSI) is frequently used to assess the stability of features 
   			and detect drift over time.
			By proactively assessing these metrics, valuable insights are gained into which features 
   			are most vulnerable to drift and how significantly model performance could deteriorate in production.
			Conducting a feature vulnerability analysis helps pinpoint features susceptible to drift. 
			By understanding these vulnerabilities, strategies can be devised to bolster 
   			model resilience and monitoring plan to mitigate risks. 

	6. Silent Uncertainty:
		Understanding where your model is uncertain isn’t just a technical curiosity — it’s a necessity.
		Uncertainty Is a Signal: high predictive uncertainty indicates model unreliability in specific 
  		regions of the input space. 
		These are areas where the model is unsure — and decisions based on its outputs are riskier.
		Find Risky Zones in the Input Space (see https://lnkd.in/eMSQkxWd): use conformal prediction intervals 
  		across many test samples to locate regions of high uncertainty. 
    
		These are typically:
			- Sparse in the training data
			- Contain overlapping class distributions
			- Have inconsistent feature patterns
   
		Conformal prediction gives statistically valid uncertainty intervals around predictions. 
  		It allows us to say: “With 90% confidence, the true value lies within this range.”
		When those ranges are wide, the model is less certain. When they’re tight, the model is confident.
		In our credit model example (see Figures below), a few clusters (0 and 7) have high uncertainty 
  		with 90% confidence, the decisions are both default and non default (width == 2: decision is both classes). 
    		Clusters 4, 5 and 8 have higher uncertainty as well. 
		In these regions, our model is unsure whether they’ll repay or default — and business risk increases.
  
		What to Do About It?
			- Flag high-uncertainty cases for manual review
			- Use uncertainty to prioritize retraining data collection
			- Adjust decision thresholds or introduce fallback rules in uncertain areas
			- Build explainability reports showing which input features 
   				(e.g., Score, DTI, Utilization) drive high uncertainty
   
		It’s Not Just About Accuracy: accuracy alone is misleading. What matters is how confident your model is 
  		when making decisions — especially in regulated, high-stakes settings like lending or healthcare.


	7. The Noise Trap—Threat of "Benign Overfitting" :
		One of the most overlooked challenges in machine learning is lack of robustness due to benign overfitting. 
  		Models often look great in development—where the train and test sets come from the same distribution—but 
    		run into trouble in production when the input noise or data distribution changes. 
      		The result? A rapid performance drop that no one saw coming.

		This figure illustrates the problem:
  
		Perturbed Model Performance (Top-Left): Notice how the AUC drops significantly under noise perturbations. 
  		Small changes in inputs can cause large swings in performance—classic fragility.
		
  		Cluster Residual (Top-Right): Clusters 0 and 8 stand out as the worst in terms of robustness, 
  		indicating these segments of the data are especially sensitive to noise.
		
  		Feature Importance (Bottom-Left): We see which features drive the fragility. 
    		“Score,” “Utilization,” and “DTI” are among the top factors contributing to the model’s noise sensitivity.

		Density Comparison (Bottom-Right): This plot highlights the problem are from Cluster 8. 
  		A shift to mid score threatens model robustness. 

		Key Takeaways:
			- Benign Overfitting can mask true risk when train and test data share the same distribution.
			- Production Noise often differs from development, triggering unexpected performance declines.
			- Identifying Fragile Clusters (like clusters 0 and 8 here) is crucial to pinpoint 
   				where the model needs improvement.
			- Understanding Feature Drivers of robustness problems (e.g., “Score,” “Utilization,” “Income”) 
   				helps us prioritize feature engineering and model tuning.

		Robustness testing—especially under varying noise conditions—is essential to ensure your model doesn’t 
  		crumble when faced with real-world data. By diagnosing where and why a model is overly sensitive, 
    		you can shore up these “holes” and build a more stable foundation for long-term success.
      

	8. Heterogeneity Blindspots—Masked Hidden Diversity:
 		One of the most critical pitfalls in machine learning is assuming that a single, 
   		monolithic model can capture all the nuances and complexities of real-world data without segmentation. 
     		In reality, data is inherently heterogeneous, and overlooking this diversity can lead to spurious 
       		relationships, poor performance, and vulnerabilities to distribution drift.

		Consider our credit modeling example: distinct sub-populations exhibit unique characteristics and 
  		risk drivers. This means that: Unconstrained Single Models: They may inadvertently capture nonsensical 
    		effects in certain clusters.

		Monotonically Constrained Models: While they enforce logical behavior, they can be too rigid to adapt 
  		to the data’s diverse nature.

		Mixture of Experts (MoE) Framework: This approach allows each “expert” to specialize in a specific 
  		sub-population, uncovering hidden diversity and leading to models that are more resilient, 
    		interpretable, and conceptually sound. See Feature Importance plots of two very distinct 
      		sub-populations below. 

		Revealing hidden diversity and addressing heterogeneity in the population is not only boosts overall 
  		performance but also enhances performance uniformity, making the model more resilient against 
    		distribution drift. For instance, in the figure provided, Cluster 0 the worst-performing region 
      		under a single model—shows marked improvement when modeled using MoE. Even the "worst" expert in the MoE 
  		framework outperforms many segments of the single model.

		Mixture of Experts is far more sophisticated than simple segmentation. While segmentation typically 
  		divides the data into static groups, MoE employs a dynamic gating mechanism that assigns varying weights 
    		to different expert models based on the input features. This adaptive process allows the model to capture 
      		subtle, continuous variations in data heterogeneity, handle overlapping regions, and respond to changes 
		in the data distribution. Instead of treating each segment as completely independent, MoE enables experts 
  		to collaborate—learning how to optimally combine their predictions for each specific input. 
    		This results in a more expressive and flexible modeling framework that uncovers hidden diversity 
      		and significantly enhances overall performance. 
	
--------------------------------------------------------------------------------------------------------------------------

	AUC measures how well a model distinguishes between two groups, Accuracy is the percentage of correct 
	predictions, F1 Score balances precision and recall, Gini is a measure of model discrimination derived 
	from AUC, and Kolmogorov-Smirnov (KS) measures the difference between two distributions. 

	1. AUC (Area Under the Curve) / ROC (Receiver operating characteristic) :
		What it is: 	AUC represents the area under the Receiver Operating Characteristic (ROC) curve,
				which plots the true positive rate (how well the model identifies positives) 
    				against the false positive rate 
				(how often the model incorrectly identifies negatives as positives) 
    				at various thresholds. Trade-off between the true positive rate (TPR) 
				and the false positive rate (FPR). Advantage of ROC curve is that 
    				it is independent of the change in the proportion of responders.
    
		How to interpret: An AUC of 1 means the model perfectly distinguishes between the two groups. 
  				An AUC of 0.5 means the model is no better than random guessing. 
				A higher AUC indicates better model performance. 

		Use Cases: AUC is particularly useful for evaluating models in scenarios with imbalanced datasets 
  				(where one group is much larger than the other) because it considers the trade-off 
      				between true and false positives. 

	2. Accuracy:
		What it is: Accuracy is the percentage of predictions that the model gets right, 
  				calculated as (correct predictions / total predictions) * 100.

		How to interpret: A higher accuracy means the model is making more correct predictions overall.
  
		Use Cases: Accuracy is a good general metric, but it can be misleading in imbalanced datasets, 
  				as a model might achieve high accuracy by simply predicting the majority class. 

	3. F1 Score:
		What it is: The F1 score is a metric that balances precision and recall, 
  				which are both important in evaluating a model's performance. 

		How to interpret: Precision is the percentage of positive predictions that were actually correct 
  				(true positives / total positive predictions). 
				Recall (also known as sensitivity) is the percentage of actual positive cases that 
    				the model correctly identified (true positives / total actual positives). 
				F1 score is the harmonic mean of precision and recall, providing a single score 
    				that considers both. Why harmonic mean and not an arithmetic mean. 
				This is because HM punishes extreme values more.	

		Use Cases: F1 score is particularly useful in scenarios where both precision and recall 
  				are important, or when dealing with imbalanced datasets. 

	4. Gini Coefficient:
		What it is: The Gini coefficient is derived from the AUC and represents the model's ability to 
  				discriminate between the two groups.
				Gini is nothing but the ratio between the area between the ROC curve and 
    				the diagonal line & the area of the above triangle.

		How to interpret: Gini = 2 * AUC - 1.
				A Gini of 0 means the model is no better than random guessing.
				A Gini of 1 means the model perfectly distinguishes between the two groups.

		Use Cases: The Gini coefficient provides a simple and intuitive measure of model performance, 
  				especially for non-technical audiences. 

	5. Kolmogorov-Smirnov (KS) Test:
		What it is: The KS test is a statistical test used to determine if two distributions are 
  				significantly different.
				KS measures the degree of separation between the positive and negative distributions. 
				The K-S is 100 if the scores partition the population into two separate groups in 
    				which one group contains all the positives and the other all the negatives.
				If the model cannot differentiate between positives and negatives, 
    				then it is as if the model selects cases randomly from the population. 
				The KS would be 0.			

		How to interpret: In the context of machine learning, the KS test can be used to evaluate 
  				the performance of a model by comparing the distribution of predicted 
      				probabilities with the distribution of actual outcomes. 
	  			A higher KS value indicates a greater difference between 
	  			the two distributions, suggesting that the model is better at distinguishing 
      				between the two groups.

		Use Cases: The KS test is useful for evaluating the performance of a model in a more nuanced way 
  				than AUC or Gini, as it considers the entire distribution of predicted probabilities. 

	Accuracy vs ROC AUC:
		- First difference is that you calculate accuracy on the predicted classes while you calculate ROC AUC 
  			on predicted scores. That means you will have to find the optimal threshold for your problem. 

		- Secondly, accuracy scores start at 0.93 for the very worst model and go up to 0.97 for the best one. 
			Remember that predicting all observations as majority class 0 would give 0.9 accuracy, 
   			so our worst experiment, BIN-98 is only slightly better than that. 
			Yet the score itself is quite high, and it shows that you should always take an imbalance 
   			into consideration when looking at accuracy. 

	F1 score vs Accuracy:
		- F1 score balances precision and recall in the positive class, while accuracy looks at correctly 
  			classified observations, both positive and negative.
			That makes a big difference, especially for the imbalanced problems, where by default 
   			our model will be good at predicting true negatives and hence accuracy will be high. 
			However, if you care equally about true negatives and true positives, 
   			then accuracy is the metric you should choose. 

	F1 score vs ROC AUC:
		- One big difference between the F1 score and the ROC AUC is that the first one 
  			takes predicted classes, and the second takes predicted scores as input. 
			Because of that, with the F1 score, you need to choose a threshold that 
   			assigns your observations to those classes. 
			Often, you can improve your model performance a lot if you choose it well.

https://www.kaggle.com/discussions/getting-started/170389

-----------------------------------------------------------------------------------------------------------------------------------
	https://www.linkedin.com/posts/agus-sudjianto-76519619_does-your-model-have-more-holes-than-swiss-activity-7307963627938992128-K6uD/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_does-your-model-have-more-holes-than-swiss-activity-7308415223051223041-3o9a/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_does-your-model-have-more-holes-than-swiss-activity-7308807687390003201-4wX1/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_does-your-model-have-more-holes-than-swiss-activity-7309182083241816065-NiGG/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_hole-5-performance-fragility-weak-resilience-activity-7309521010972655616-zDH0/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_hole-6-silent-uncertaintywhich-parts-of-activity-7309959171263660032-wBkg/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_hole-7-the-noise-trapthreat-of-benign-activity-7310308433537171456-fIJE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAJdJL4BriNQhSVKeKIC1Ppfn_ZfLrujGCo

	https://machinelearningmastery.com/how-to-code-the-students-t-test-from-scratch-in-python/
	Stock-en-Presentation-Agri-test-CCAR-Work-Risk-FRM

-----------------------------------------------------------------------------------------------------------------------------------
