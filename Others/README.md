------------------------------------------------------------------------------------------------------
# Type of data

### Nominal / categorical/qualitative /non-parametric:  
		Example	: 	colour,gender. 
		check	: 	frequency each category. 
		Test	: 	Comparison/Difference: 
					- Test for proportion (for one categorical variable)
					- Difference of two proportions
					- Chi-Square test for independence (for two categorical variables)
		Relationship:	Chi-Square test for independence
	
	
### Ordinal : Similar as Nominal
		Example	:	rank, satisfaction.  
		Check	: 	frequency or mean (special case). 
		Test	:	Similar as Nominal
		
		
### Interval / Ratio / quantitative/continuous  : 
		Example	:	number of customers, income,age. 
		Check	: 	Mean ..
		Test	:	Comparison/Difference:
					- Test for a mean / T test (for one continuous variable)
					- Difference of two means (independent samples)
					- Difference of two means(paired T test. Pre and Post scenario)
		Relationship:	Regression Analysis / Correlation (for two continuous variables/for relationship)
			
		
#### One categorical and one continuous: T test (Anova when more than 2 category)
	https://www.youtube.com/watch?v=tfiDu--7Gmg
	
### Imbalanced data
		Imbalanced data refers to those types of datasets where the target class has an uneven 
		distribution of observations, i.e one class label has a very high number of observations 
		and the other has a very low number of observations (rare event i.e., Fraud)
------------------------------------------------------------------------------------------------------

## Sampling

#### Under-sampling: 
- Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modelling.

#### Over-sampling: 
- On the contrary, oversampling is used when the quantity of data is insufficient. It tries to balance dataset by increasing the size of rare samples. Rather than getting rid of abundant samples, new rare samples are generated by using e.g. repetition, bootstrapping or SMOTE (Synthetic Minority Over-Sampling Technique).

#### Use K-fold Cross-Validation in the right way: 
- Cross-validation should be applied properly while using over-sampling method to address imbalance problems. Keep in mind that over-sampling takes observed rare samples and applies bootstrapping to generate new random data based on a distribution function. If cross-validation is applied after over-sampling, basically what we are doing is overfitting our model to a specific artificial bootstrapping result. That is why cross-validation should always be done before over-sampling the data, just as how feature selection should be implemented. Only by resampling the data repeatedly, randomness can be introduced into the dataset to make sure that there won’t be an overfitting problem.

https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html

#### Resampling:
- Consider testing under-sampling when you have an a lot data 
- Consider testing over-sampling when you don’t have a lot of data 
- Consider testing random and non-random (e.g. stratified) sampling schemes.
- Consider testing different resampled ratios

#### Synthetic Samples
- SMOTE or the Synthetic Minority Over-sampling Technique: SMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances.

#### Borderline SMOTE, SVM-SMOTE

#### Balanced Bagging Classifier: 
- When we try to use a usual classifier to classify an imbalanced dataset, the model favors the majority class due to its larger volume presence. A Balanced Bagging Classifier is the same as a sklearn classifier but with additional balancing.

#### Cluster the abundant class: 
- An elegant approach was proposed by Sergey on Quora. Instead of relying on random samples to cover the variety of the training samples, he suggests clustering the abundant class in r groups, with r being the number of cases in r. For each group, only the medoid (centre of cluster) is kept. The model is then trained with the rare class and the medoids only.

---------------------------------------------------------------------------------------------

## Hypothesis Testing

- Way to figure out believe is likely true based on a sample.
	- Start with a belief about the larger group. This belief is hypothesis.
	- Collect data from a sample of that group. 
	- Use statistical methods to compare sample data to initial belief and see how likely belief is true.
	- Drawing a conclusion based on the statistical analysis.
		- Accept or fail to reject the belief: This means there's not enough evidence to say belief is wrong.
		- Reject the belief: This means sample data provides strong enough evidence that belief is likely false.

- Example:

	- A telecom service provider claims that customers spend an average of ₹400 per month, with a standard deviation of ₹25. However, a random sample of 50 customer bills shows a mean of ₹250 and a standard deviation of ₹15. Does this sample data support the service provider’s claim?

	- Null Hypothesis (H0): The average amount spent per month is ₹400.
	- Alternate Hypothesis (H1): The average amount spent per month is not ₹400.

	- Population mean (m): ₹400
	- Population Standard Deviation (σ): ₹25
	- Sample Size (n): 50
	- Sample Mean (x̄): ₹250
	- Significance level: 5%

![image](https://github.com/user-attachments/assets/f3c88d3e-9129-4d9f-a4f4-60811faade31)
    
	1. Calculate the z-value: z=(x̄-m)/{σ/sqrt(n)}
		z = (250 - 400)/ {25/SQRT(50)} =  (-150) / { 25 / 7.07 } = -150 / 3.54 = −42.42
	2. Compare with critical z-values:
		- For a 5% significance level, critical z-values are -1.96 and +1.96.
		- Since -42.42 is far outside this range, reject the null hypothesis.
	The sample data suggests that the average amount spent is significantly different from ₹400.

------------------------------------------------------------------

- Hypothesis testing uses data from a sample to draw conclusions about a population parameter or a population probability distribution.  It's a statistical method used to determine if there's enough evidence in a sample to reject a null hypothesis in favor of an alternative hypothesis. It help to make better decisions.

- Avoiding Misleading Conclusions (Type I and Type II Errors)
	- It helps you avoid jumping to the wrong conclusions.
   	- For instance, a Type I error could occur if a bank launches a new product thinking it will be a hit, only to find out later that the data misled them.
  	- A Type II error might happen when a bank overlooks a potentially successful product because their testing wasn’t thorough enough.
- By setting up the right significance level and carefully calculating the p-value, hypothesis testing minimizes the chances of these errors, leading to more accurate results.

- Making Smarter Choices:
  	- Evidence-based decisions. Let’s say a bank wants to determine if new strategy will increase revenue.
  	- By testing the hypothesis using data from similar projects, they can make an informed choice.
  	- Similarly, a bank might use hypothesis testing to see if a credit limit increse actually improves credit card spending.
  	- It’s about taking the guesswork out of decisions and relying on solid evidence instead.

- Optimizing Business Tactics:
  	- In business, hypothesis testing is invaluable for testing new ideas and strategies before fully committing to them.
  	- For example, bank company might want to test whether offering upgraded credit card increases spending.
  	- By using hypothesis testing, bank can compare spend data from customers who received upgraded credit card vs who didn’t.
  	- This allows bank to base decisions on data, not hunches, reducing the risk of costly mistakes.
  
- Null Hypothesis (H0): This hypothesis states that there is no effect or difference, and it is the hypothesis you attempt to reject with your test.
- Alternative Hypothesis (H1 or Ha): This hypothesis is what you might believe to be true or hope to prove true. It is usually considered the opposite of the null hypothesis.
  
- Significance level: A significance level is used to determine if the null hypothesis is true or not. Often denoted by alpha (α), is the probability of accurately rejecting the null hypothesis. 
  Common choices for α are 0.05 (5%), 0.01 (1%), and 0.10 (10%). The significance level is the probability of a Type I error, i.e., rejecting a null hypothesis that is true.
  
- Confidence level:
	- The confidence level indicates the probability of obtaining the same results if you repeat the same data collection processes like tests, polls, or surveys.
	- The confidence level is established before conducting the data integration and collection, typically between 90% and 99%. It helps you determine whether the results from sample data apply to the entire population.
	- The confidence level and significance level are related. If you have a 95% confidence interval, it corresponds to a 5% significance level in the hypothesis test.
	- Confidence level indicates the probability of drawing accurate conclusions based on sample data. The significance level is a concept that deals with testing a hypothesis and avoiding a type I error, while the confidence level deals more with the precision of the results despite the repetition of the test. These two concepts have an inverse relationship, meaning that if the significance level increases,
  	the confidence level decreases, and vice versa. If you want to be more certain of the conclusions you draw, your confidence level needs to be higher.
	- Both confidence level and significance level are used to determine the confidence interval.  Confidence level = 1-Significance level

![Function](https://github.com/amitmse/in_Python_/blob/master/Others/Significance%20Level%20Confidence%20Level.png)

- Confidence Intervals:
	- It is a range of values within which the true population value likely falls, with a certain level of confidence.
	- Think of it as an estimate, but instead of just giving a single number, it provides a range.
	- This range is calculated from a sample of data and tells you how accurately your sample represents the entire population.
   
- Margin of Error:
	- The width of the confidence interval is related to the margin of error, which indicates how much your sample estimate might vary from the true population value. 

- P-value:
  
	- P-value represents the probability that observed results would occur if the null hypothesis were true. The null hypothesis typically assumes there's no effect or no difference between groups.
   
	- Probability of observing test results at least as extreme as the results observed, assuming the null hypothesis is correct.
 	- It helps determine the strength of the evidence against the null hypothesis.
  	- P value is compared with a significance level. It does not provide information about the magnitude of the effect. 
	- The p-value is calculated based on the data and the test statistic (e.g., t-statistic, z-statistic) used to test the hypothesis.
	- Once test statistic "t" calculted, find the associated p-value by referring to a t-distribution table.
  
	- If the p-value ≤ α: Reject the null hypothesis, suggesting sufficient evidence in the data supports the alternative hypothesis.
	- If the p-value > α: Do not reject the null hypothesis, suggesting insufficient evidence to support the alternative hypothesis.

- Critical region:
	- All sets of values that lead to rejecting the null hypothesis lie in the critical region. Critical value separates the critical region from the non-critical region.
	- Critical value is the value of the test statistic which defines the upper and lower bounds of a confidence interval.
  
- One-Tailed test Hypothesis Testing:
	- Also called a directional test. critical distribution area is one-sided, meaning the test sample is either greater or lesser than a specific value.
- Two tails:
	- the critical distribution area is two-sided.
  
- Statistical power refers to the probability that a statistical test will correctly reject a false null hypothesis.

- Limitations of Hypothesis Testing
	- It cannot prove or establish the truth: Hypothesis testing provides evidence to support or reject a hypothesis, but it cannot confirm the absolute truth of the research question.
	- Results are sample-specific: Hypothesis testing is based on analyzing a sample from a population, and the conclusions drawn are specific to that particular sample.
	- Possible errors: During hypothesis testing, there is a chance of committing type I error (rejecting a true null hypothesis) or type II error (failing to reject a false null hypothesis).
		- Type 1 Error: A Type-I error occurs when sample results reject the null hypothesis despite being true.
		- Type 2 Error: A Type-II error occurs when the null hypothesis is not rejected when it is false, unlike a Type-I error.
  
-Assumptions and requirements: Different tests have specific assumptions and requirements that must be met to accurately interpret results.

- Degrees of Freedom
	- It refer to the number of independent pieces of information available when calculating a statistic.
	- Trying to find the average of a group of numbers given total sum and the count of numbers are know.
	- It means the last number is automatically determined for average computation, meaning it doesn't have freedom to vary independently.
	- So, the degrees of freedom would be one less than the count of numbers, as freely choose all but the last one.

---------------------------------------------------------------------------------------------

# Probability Distribution

- Probability distributions function (PDF) describe what we think the probability of each outcome is.
- probability mass function is used to describe the probabilities of discrete random variables,
  while the PDF is used to describe the probabilities of continuous random variables.
- They come in many shapes, but in only one size: probabilities in a distribution always add up to 1.
- A probability distribution is a function that assigns to each event a number in [0,1] which is 
  the probability that this event occurs.
- PDF shows the probability at a single point, and the cumulative distribution function (CDF) shows the cumulative probability up to that point.
- A statistical model is a set of probability distributions. We assume that the observations are generated 
  from one of these distributions.
- Why Probability Density and why not Probability: The concept of probability density is used for continuous random variables because in such cases, the probability of any specific value is infinitesimally small.
  This is because the number of possible values that a continuous random variable can take is infinite, making it impossible to assign a non-zero probability to any individual value.
  Instead, we use probability density to describe the distribution of continuous random variables.
- The probability distribution shows the probability of a point, and the CDF tells the probability of everything up to that point.  
- Chart: Horizontal axis set of possible numeric outcomes. Vertical axis probability of outcomes.
- Example:
	- Flipping a fair coin has two outcomes: it lands heads or tails. 
	- Before the flip, we believe there’s a 0.5 probability, of heads and same for tails. 
	- That’s a probability distribution over the two outcomes of the flip (Bernoulli distribution).
	
![Function](https://github.com/amitmse/in_Python_/blob/master/Others/distribution.png)

- Continuous Probability Distribution: Gaussian Normal, Standard Normal (Z- dist.), Student-t, Uniform, Log-Normal, Chi-Square.
- Discrete Probability Distribution: Bernoulli, Binomial, Negative Binomial, Geometric, Poisson, Uniform (defined both).

---------------------------------------------------------------------------------------------
## Uniform distribution

- Many equally-likely outcomes (Bernoulli):the uniform distribution, characterized by its flat PDF. 
- It can be defined for any number of outcomes or even as a continuous distribution.
- Function
	- PDF		: 1/(b-a) 	       {-infinite <- (a,b) -> infinite}
	- Mean  	: (a+b)/2
	- Variance 	: (b-a)^2/12
Example: 
	- Imagine rolling a fair die. The outcomes 1 to 6 are equally likely.

---------------------------------------------------------------------------------------------
## Bernoulli distribution

- Bernoulli distribution has only two possible outcomes i.e. success and failure in a single trial
- The Bernoulli PDF has two lines of equal height, representing the two equally-probable outcomes of 0 and 1 at either end.
- Bernoulli Distribution is a special case of Binomial Distribution with a single trial
- Function
	- PDF		: P^x*(1-P)^(1-x)       {x in 0 or 1}
	- Mean  	: P
	- Variance 	: P(1-P)
- Example: 
	- Flipping a fair coin
	- it’s going to rain tomorrow or not

---------------------------------------------------------------------------------------------
## Binomial distribution

- The binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution.
- Function
	- PDF		: [n!/(n-x)!*x!] * [P^x*(Q)^(n-x)]	{! factorial}
	- Mean  	: nP
	- Variance 	: nPQ
- Example: 
	- Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows 
	  the binomial distribution. Each flip is a Bernoulli-distributed outcome. Converted to binomial 
	  distribution when counting the number of successes, where each flip is independent and has 
	  the same probability of success.
	
	- Imagine an urn with equal numbers of white and black balls. Draw a ball and note whether it is black, 
	  then put it back and Repeat this process. How many times black ball was drawn? 
	  This count also follows a binomial distribution.

-------------------------------------------------------------------------------------------------
## Logistic distribution

- The logistic distribution is a continuous probability distribution similar in shape to the normal distribution but with heavier tails.
  Heavier tail is due to its mathematical structure. While logistic and normal distribution both are symmetric and unimodal.
  The logistic distribution's PDF decays more slowly in the tails compared to the normal distribution,
  meaning there's a higher probability of observing values further from the mean.
  This characteristic makes the logistic distribution more robust to outliers and extreme values.
  
- sigmoid function  / logistic function (Cumulative Distribution Function) =  1 / (1+exp^-y)
- Probability Density Function: exp^-y / (1 + exp^-y)^2
- Mean = 0
- Variance = Phi^2 / 3
  
- The logistic distribution is a probability distribution (PDF), and its cumulative distribution function (CDF) is the sigmoid function.
- It's used in various fields, including logistic regression, neural networks, and modeling growth patterns.
- The key feature is that its cumulative distribution function (CDF) is the logistic function,
  which is crucial in mapping real numbers to probabilities, especially in machine learning applications.
- The distribution is symmetrical around its mean. The mode and median of the logistic distribution are equal to its mean.
- Below chart is Probability Density Function. CDF is S shaped.

https://www.acsu.buffalo.edu/~adamcunn/probability/standardlogistic.html

![Function](https://github.com/amitmse/in_Python_/blob/master/Others/Logistic-%20Normal.png)

---------------------------------------------------------------------------------------------	
## Hyper-Geometric distribution

- Example: 
	- This is the distribution of that same count if the balls were drawn without replacement instead. 
	  Undeniably it’s a cousin to the binomial distribution, but not the same, because the probability 
	  of success changes as balls are removed. 
	- If the number of balls is large relative to the number of draws, the distributions are similar
	  because the chance of success changes less with each draw.
	
---------------------------------------------------------------------------------------------	
## Poisson distribution

- Simialr to the binomial distribution, the Poisson distribution is the distribution of a 
  count - the count of times something happened. 
- The Poisson distribution is when trying to count events over a time given the continuous rate of events occurring
- Poisson Distribution is a limiting case of binomial distribution.
  As n approaches ∞ and p approaches 0 with the product np held fixed, the Binomial (n, p) distribution approaches the Poisson distribution with expected value λ = np.
- Function								
	- PDF		: Expo(-Mean)*{(Mean^x)/x!} 		{x in o,1,2,3,4,5}
	- Mean  	: Mean
	- Variance 	: Mean
- Example:
	- Packets arrive at routers, or customers arrive at a store, or things wait in some kind of queue Count 
	  of customers calling a support hot-line each minute doesn't follow binomial/Bernoulli but Poisson.
	- The number of emergency calls recorded at a hospital in a day.
	- The number of thefts reported in an area on a day.
	- The number of customers arriving at a salon in an hour.
	- The number of suicides reported in a particular city.
	- The number of printing errors at each page of the book.
	- The number of incoming calls at a call center in a day.
	
---------------------------------------------------------------------------------------------	
## Geometric distribution

- If the binomial distribution is “How many successes?” then the geometric distribution is
  “How many failures until a success?”
- Example:
	- From simple Bernoulli trials arises another distribution. How many times does a flipped coin 
	  come up tails before it first comes up heads? This count of tails follows a geometric distribution.

---------------------------------------------------------------------------------------------
## Negative Binomial distribution

- It's a simple generalization. It’s the number of failures until r successes have occurred,not just 1.
- Example:
	- If we flip a coin a fixed number of times and count the number of times the coin turns out heads is a binomial distribution.
   		If we continue flipping the coin until it has turned a particular number of heads say the third head-on flipping 5 times, then this is a case of the negative binomial distribution.
   	- For a situation involving three glasses to be hit with 7 balls, the probability of hitting the third glass successfully with the seventh ball can be obtained with the help of negative binomial distribution.
	- In a class, if there is a rumor that there is a math test, and the fifth is the second person to believe the rumor, then the probability of this fifth person to be the second person to
   		believe the rumor can be computed using the negative binomial distribution.

---------------------------------------------------------------------------------------------  
## Exponential distribution

- The exponential distribution is one of the widely used continuous distributions. 
- It is often used to model the time elapsed between events.
- The exponential distribution should come to mind when thinking of "time until event", maybe "time until failure".
- Exponential distribution is widely used for survival analysis. From the expected life of a machine to 
  the expected life of a human, exponential distribution successfully delivers the result.
- There is a strong relationship between the Poisson distribution and the Exponential distribution.
- Function								
	- PDF		: λe^(-λx)    		{x ≥ 0}
	- Mean  	: 1/λ
	- Variance 	: (1/λ)²
- Example: 
	- let’s say a Poisson distribution models the number of births in a given time period. 
	  The time in between each birth can be modeled with an exponential distribution.
	- Duration of a telephone call
	- How long does it take to perform a service, fix something at a service point etc.
	- Duration between two phone calls
	- Half life of atoms (radioactive decay)
	- Expected lifetime of electronic (or other) parts, 
	  if wearing is not considered (this is called Mean Time Between Failures, MTBF)
	- Age of plants or animals
	- Very simple model used by insurance companies

---------------------------------------------------------------------------------------------
## Weibull

- Weibull distribution can model increasing (or decreasing) rates of failure over time. 
- The exponential is merely a special case.
- Commonly used to assess product reliability, analyze life data and model failure times
- Weibull isn’t an appropriate model for every situation i.e. chemical reactions and corrosion failures are 
  usually modeled with the lognormal distribution.

---------------------------------------------------------------------------------------------
## Normal Distribution / Gaussian distribution

- The sum of Bernoulli trials follows a binomial distribution, and as the number of trials increases, 
  that binomial distribution becomes more like the normal distribution. 
- Its cousin the hyper-geometric distribution does too. 
- The Poisson distribution—an extreme form of binomial—also approaches the normal distribution as 
  the rate parameter increases.
- The mean, median and mode of the distribution coincide.
- The curve of the distribution is bell-shaped and symmetrical about the line x=μ.
- Normal distribution is another limiting form of binomial distribution.
- As n approaches ∞ while p remains fixed, the distribution of approaches the normal distribution with expected value 0 and variance 1.
  This result is sometimes loosely stated by saying that the distribution of X is asymptotically normal with expected value 0 and variance 1.
  This result is a specific case of the central limit theorem.
- Its popular due to Central Limit Theorem.
  The central limit theorem states that under certain (fairly common) conditions, the sum of many random variables will have an approximately normal distribution.
  The logarithm of various variables tend to have a normal distribution, that is, they tend to have a log-normal distribution 
- Function								
	- PDF		: [1/{SQRT(2Pai)*STD}]*Expo[(X-Mean)^2/-2Variance]   {-infinite <-x-> infinite}
	- Mean  	: Mean
	- Variance 	: Variance
- Example:
	- Heights of people, Measurement errors, Blood pressure, Points on a test, IQ scores, Salaries.


- In the Bayesian approach, all uncertainty is measured by probability.
  The Bayesian approach assumes the 'a priori' knowledge of probability models, in such a way that it is possible to build exact models of phenomena starting
  from experimental data, and then use the models to make predictions.
---------------------------------------------------------------------------------------------
### Standard Normal distribution

- It is also known as the Z distribution and it follows normal distribution with a mean of zero and a variance of one.
  The Standard Normal Distribution is used to standardize and compare different normal distributions by converting them into a single, common reference.
  This allows for easier calculation of probabilities and comparison of data points across various datasets.
  It also helps in hypothesis testing and identifying the probability of sample means significantly differing from population means.
  The standard normal distribution is a specific type of normal distribution with a mean of 0 and a standard deviation of 1. It is often used to calculate z-scores and probabilities.
  
  z = (x - μ) / σ  (x is raw value, μ mean and σ standard deviation)

  - Example: 
	- For example, if you get a score of 90 in Math and 95 in English, you might think that you are 
  	  better in English than in Math. However, in Math, your score is 2 standard deviations above 
	  the mean. In English, it’s only one standard deviation above the mean. It tells you that in Math, 
  	  your score is far higher than most of the students (your score falls into the tail).

---------------------------------------------------------------------------------------------
### Z-test

- The sample is assumed to be normally distributed. A z-score is calculated with population parameters 
  such as "population mean" and "population standard deviation" and is used to validate a hypothesis 
  that the sample drawn belongs to the same population. Sample mean is same as the population mean.
- Function								
	- PDF		: (x - mean)/standard deviation
	- Mean  	: 0
	- Variance 	: 1			  

---------------------------------------------------------------------------------------------
## t /Student  Distribution

- The t test tells how significant the differences between groups are. A t-test is used to compare the mean of 
  two given samples.
- A t-test is used when the population mean and population standard deviation are unknown.
- Independent samples t-test which compares mean for two groups. t test for equality of 
  population mean when variance is same.
- Before t test, F test is required for equality for variance.
- One sample t-test which tests the mean of a single group against a known mean.
- Test the significance of regression coefficient. 
- Example:
	- A very simple example: Let’s say you have a cold and you try a naturopathic remedy. 
	  Your cold lasts a couple of days. The next time you have a cold, you buy an over-the-counter 
	  pharmaceutical and the cold lasts a week. You survey your friends and they all tell you that 
	  their colds were of a shorter duration (an average of 3 days) when they took the homeopathic remedy. 
	  What you really want to know is, are these results repeatable? A t test can tell you by comparing
	  the means of the two groups and letting you know the probability of those results happening by chance.

	- Paired sample t-test which compares means from the same group at different times. 
	  Choose the paired t-test if you have two measurements on the same item, person or thing
		
---------------------------------------------------------------------------------------------		
## Chi-Squared Distribution 

- Tests for the strength of the association between two categorical variables. Chi Square lets you know whether 
  two groups have significantly different opinions, which makes it a very useful statistic for survey research.
- Population mean is known and test the variance of normal distributed. chi squared distribution is the square 
  of a normal distribution.
- Function
	- PDF		: (Observed - Mean)^2/Mean
	- Mean  	: mean
- Example:
	- The chi-squared distribution is used primarily in hypothesis testing
	- Goodness of fit test, which determines if a sample matches the population 
		(does a coin tossed 20 times turn up 10 heads and 10 tails?)
	- A chi-square fit test for two independent variables is used to compare two variables in a contingency table 
		to check if the data fits
	- Chi-squared test of independence in contingency tables (is there a relationship between gender and salary?)
	- Likelihood-ratio test for nested models
	- Log-rank test in survival analysis
	- Cochran–Mantel–Haenszel test for stratified contingency tables

---------------------------------------------------------------------------------------------
## Likelihood-ratio

- This test assesses the goodness of fit of two competing statistical models based on the ratio of their likelihoods

---------------------------------------------------------------------------------------------
## F-test

- F-test of equality of variances is a test for the null hypothesis that two normal populations have the same variance. 

- It is most often used when comparing statistical models that have been fitted to a data set, 
  in order to identify the model that best fits the population from which the data were sampled. 
  "F-tests" mainly arise when the models have been fitted to the data using least squares.

-----------------------------------------------------------------------------------------------------------------------------------
	https://medium.com/@srowen/common-probability-distributions-347e6b945ce4
	https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/
	https://www.johndcook.com/blog/distribution_chart/
	https://www.statisticshowto.datasciencecentral.com/probability-distribution/
-----------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------

## Covariance

- It refers to the measure of how two variables will change (directional relationship) when they are compared to each other
- It measures the Variance between two variables
- Covariance indicates the direction of the linear relationship between variables. Correlation on the other hand measures both the strength and direction of the linear relationship between two variables. 

	Covariance(X,Y) = E[(X−EX)(Y−EY)] = E[XY]−(EX)(EY)

 	correlation(x,y)= covariance(x,y) / [variance(x)*variance(y)]

## Correlation vs Regression

- Correlation: 
	- It measures the degree of relationship between two variables. 
	- correlation doesn’t capture causality.
	- Correlation between x and y is the same as the one between y and x.
- Regression analysis 
	- It is about how one variable affects another or what changes it triggers in the other.
	- Regression is based on causality (cause and effect).
	- Regression of x and y, and y and x, yields completely different results.
	
## ANOVA

- Known as analysis of variance, is used to compare multiple (three or more) samples with a single test 
	i.e. all sample means are equal
	
------------------------------------------------------------------------------------------------------

# Ordinary Least Squares (OLS)

- Finds parameter values that minimizing the error.
 
- Gauss-Markov Theorem: It provides a theoretical justification for using OLS in linear regression, guaranteeing that the resulting estimates are BLUE. 
- Best Linear Unbiased Estimator (BLUE) refers to the Ordinary Least Squares (OLS) estimator. Why OLS is BLUE:
	- Most Efficient (Best): Because OLS has the smallest variance, it is considered the most efficient estimator for estimating the regression coefficients. 
	- Linear: An estimator is linear.   
	- Unbiasedness: The OLS estimator is unbiased under the Gauss-Markov assumptions. 
	- Minimizes Variance: The Gauss-Markov theorem guarantees that the OLS estimator has the smallest variance among all unbiased linear estimators. 

## Assumptions of Linear regression: 6

### For Model:

1. Linear in parameters :
   
	- Issue	: Beta not multiplied or divided by any other parameter. Incorrect and unreliable model which leads to error in result.
	- Solution: Transformations of independent variables

### For Variable: 

2. No perfect multicollinearity :
   
	- Issue: Regression coefficient variance will increase.
  
	  It inflates standard errors, leading to unstable and unreliable coefficient estimates, making it difficult to determine the individual effect of each predictor on the outcome and potentially leading to misleading conclusions about variable importance.
		
	  The presence of multicollinearity makes the coefficient estimates highly sensitive to small changes in the data or model specification. Even a slight shift in the data can cause the coefficients to fluctuate significantly, making the model's interpretation unreliable.

	  Multicollinearity does not directly affect the goodness-of-fit statistics of the model, such as R-squared or the overall F-test. The model can still make accurate predictions despite the presence of multicollinearity.

	- Test: VIF

		VIF: [1.0 / (1.0 - R Squared)]

		VIF = 1/T  (T refers to Tolerance = 1 – R² which is unexplained portion)

			R^2 is regressing each independent variable on the other independent variables.
  
		Tolerance: It measures the influence of one independent variable on all other independent variables.
			It measures of how much a predictor variable's variance is not explained (1 – R²) by 
			the other predictor variables in the model. 
			It essentially assesses the degree to which a variable is independent of the others.
           
	- Solution: Transformations of independent variables
		
### For Error Tearm: 4

3. Normality of residuals :

	Differences between observed and predicted values.

	In linear regression, the assumption that "errors follow a normal distribution" means the difference between the observed values and the values predicted by the model (the residuals) are assumed to be distributed according to a normal distribution. 
     
	- Issue: OLS estimators won’t have the desirable BLUE property. 
  
	  This assumption is crucial for the validity of statistical tests used in linear regression, such as hypothesis tests, t-tests, p-values and ANOVA, which rely on the normality of the errors to provide accurate regression coefficients and confidence intervals.
  
	  Standard errors and t-values used to calculate the statistical significance of regression parameters may be inaccurate, leading to potentially misleading conclusions.

	- Test: Jarque-Bera test, Kolmogorov-Smirnov Test, Shapiro-Wilk test, histograms or Q-Q plots
    
	- Solution: Transforming the dependent or independent variables, Use robust statistical methods that are less sensitive to non-normality may be appropriate.
    
5. Homoscedasticity of residuals /equal variance of residuals:
   
	Homoscedasticity in the context of residuals refers to the assumption that the variance of the error terms (residuals) is constant across all levels of the independent variables in a regression model.

	- Issue: Homoscedasticity is necessary for accurately estimating the standard errors of the regression coefficients. The standard errors of the coefficients may be biased, leading to unreliable hypothesis tests nd confidence intervals. Homoscedasticity ensures that the estimated coefficients are unbiased and have minimum variance.
  
	  Example	: Family income to predict luxury spending. Residuals are very small for low values of family income (less spend on luxury) while there is great variation in the size of the residuals for wealthier families. Standard errors are biased and it leads to incorrect conclusions about the significance of the regression coefficients
     
	- Test	: Breush-Pagan test, Goldfeld-Quandt, Koenker-Bassett (generalized Breusch-Pagan)
		Breush-Pagan: Calculate the Square the residuals, and Regress it on the independent variables.

  	- Solution: Weighted least squares regression (heavier weights given to smaller error variances) Transform the dependent variable using one of the variance stabilizing transformations
	
7. No autocorrelation (serial correlation) of residuals :
   
	- Issue: correlation with own lag (stock price today linked with yesterday's price). If above fails then OLS estimators are no longer the Best Linear Unbiased Estimators. While it does not bias the OLS coefficient estimates, the standard errors tend to be underestimated (t-scores overestimated) when the autocorrelations of the errors at low lags are positive. Low variance in unbiased estimator. Autocorrelation can lead to biased and unreliable standard errors, affecting the validity of statistical tests and confidence intervals. Potentially missing key variables or an incorrect functional form. 
   
	- Test :  Durbin-Watson Test: This test checks for autocorrelation of order one (correlation between consecutive residuals). This test ranges from 0 to 4.
			2: Indicates no autocorrelation. 
   			< 2: positive autocorrelation 
			> 2: Indicates negative autocorrelation.
	Breusch-Godfrey Test: This test is designed to detect autocorrelation of any order in the residuals. Autocorrelation Function (ACF) Plot: This plot displays the correlation of the residuals with their lagged values, helping to identify the lag order of autocorrelation Plotting the residuals over time can reveal patterns or trends, indicating autocorrelation. 

	- Solution: Generalized Least Squares (GLS), Include lagged values of the dependent variable or independent variables in the regression model. Transform the Data: Apply mathematical transformations to the data to reduce autocorrelation. Use Time Series Models: If the data is time-series, consider using models specifically designed for time series analysis, such as ARIMA models      

	OLS vs GLS: OLS assumes the error is independent, identically distributed, and have constant variance. GLS relaxes above assumption and allows for heteroscedasticity, and autocorrelation.

9. Mean of residuals is zero :
    
	The sum of the residuals is always zero when an intercept is included in the regression model. This is because the regression line is fitted in a way that the total distance of the data points above the line is equal to the total distance of the data points below the line. If a model without an intercept is used, the residuals will not necessarily have a mean of zero. The mean of residuals is zero is a direct result of how the least squares method calculates the regression line. It's not an assumption that needs to be tested.
  
	- Issue: Error terms has zero mean and doesn’t depend on the independent variables. Thus, there must be no relationship between the independent variable and the error term. A model with a zero mean for residuals suggests that the model is, on average, neither overestimating nor underestimating the response variable.
   
	- Test: Plot of residuals against the fitted values. If the residuals are randomly scattered around zero, with no apparent trend, it suggests that the mean-zero assumption is met. A flat, horizontal line at zero in this plot would indicate a good fit. This can be done by calculating the mean of the residuals and comparing it to zero.
  
	- Solution: The sum of residuals can always be zero; if they had some mean that differed from zero you could make it zero by adjusting the intercept by that amount. 
 
11. X variables and residuals are uncorrelated 
	
12. Number of observations must be greater than number of Xs

##### Linear model should have residuals mean zero, have a constant variance, and not correlated with themselves or other variables. If these assumptions hold true, the OLS procedure creates the best possible estimates.

------------------------------------------------------------------------------------------------------

### Limitation of Linear Regression

- OLS assumptions are limitation.
- Linearity Assumption: Linear relationship between the independent and dependent variables. In real-world relationships are nonlinear, and forcing a linear model onto such data can lead to inaccurate predictions.
- Sensitivity to Outliers: Outliers, or extreme data points, can disproportionately influence the regression line, leading to inaccurate models.
- Multicollinearity: When independent variables are highly correlated with each other, leading to stability and precision issue in the model.
- Underfitting: when the model is too simple and fails to capture the underlying relationships in the data.
- Overfitting: when the model is too complex and learns the training data too well, including noise, leading to poor generalization to new data.
- Dependence Assumption: All independent variables are independent of each other.

------------------------------------------------------------------------------------------------------
## Logistic regression 

https://github.com/amitmse/in_Python_/tree/master/Logistic%20Regression

## Gradient Descent
https://github.com/amitmse/in_Python_/blob/master/Others/README.md#shapley-shapley-additive-explanations

--------------------------------------------------------------------------------------	
## Decision Tree 

https://github.com/amitmse/in_Python_/blob/master/Decision%20Tree/README.md

- Gini Index: https://github.com/amitmse/in_Python_/blob/master/Decision%20Tree/README.md#gini-index
- Chi-Square: https://github.com/amitmse/in_Python_/blob/master/Decision%20Tree/README.md#information-gain
- Information Gain: https://github.com/amitmse/in_Python_/blob/master/Decision%20Tree/README.md#information-gain
- Reduction in Variance: https://github.com/amitmse/in_Python_/blob/master/Decision%20Tree/README.md#information-gain

------------------------------------------------------------------------------------------------------

## Mean Decrease in Accuracy (MDA) / Accuracy-based importance / Permutation Importance:

https://github.com/amitmse/in_Python_/blob/master/Random_Forest/README.md#mean-decrease-in-accuracy-mda--accuracy-based-importance--permutation-importance

## Gini Importance / Mean Decrease in Impurity (MDI) :

https://github.com/amitmse/in_Python_/blob/master/Random_Forest/README.md#gini-importance--mean-decrease-in-impurity-mdi-

------------------------------------------------------------------------------------------------------	
# Random Forest
https://github.com/amitmse/in_Python_/tree/master/Random_Forest#readme

------------------------------------------------------------------------------------------------------	

## Bias - Variance

### Bias: 

- If model is too simple and has very few parameters then it may have high bias and low variance.
- It creates underfitting problem. High error on training and test data.
- Bias is the difference between the prediction of model and actual value. 
- Check bias in human decision-making is carried over to the development. The data-generating process itself can be biased. One way to identify data bias is by benchmarking with other models. Random selection of development sample.
- Assess model's bias (error due to assumptions).

### Variance: 

- It creates overfitting problem. Model performs very well on development data but poor performance on validation.
- If model has large number of parameters then it’s going to have high variance and low bias.
- For high variance, one common solution is to reduce parameter/features.
- This very frequently increases bias, so there’s a tradeoff to take into consideration.
- Assess model's Variance (sensitivity to training data fluctuations).

-----------------------------------------------------------------------------------------------------------------------

### Regularization

https://github.com/amitmse/in_Python_/tree/master/Regularization

-----------------------------------------------------------------------------------------------------------------------

### Cluster Analysis

https://github.com/amitmse/in_Python_/tree/master/Cluster%20Analysis

-----------------------------------------------------------------------------------------------------------------------

# Model Validation:

-------------------------

#### SR 11-7: 

- Regulators expect ML models to comply with the standards of SR 11-7 part of model risk management (MRM).
- Conceptual Soundness: 
	- Assess model design, construct, variable selection, decisions based on incorrect info, empirical evidence, documentation, limitations, misuse of model outputs.
	- Any good model always has weakness or hidden weaknesses.
	- Where does this model fail and under what conditions?	
	- How can it be exploited or misused?
	- Find a blind spots before they become costly failures
- Data Integrity/Representativeness: 
	- The data used for model development be representative of the bank’s portfolio and market/business conditions
- Validation:
	- Out of time validation. 
	- Also k-fold cross-validation, Stratified K-Fold, time-based splits can be used.
	- Validation guides model refinement during development, and testing validates its performance in real-world contexts, ensuring it behaves reliably and effectively beyond the training data.
   
--------------------------------

#### SS 1/23:
- Model development, implementation and use
- Independent model validation
- Model risk mitigants 
- model risk classification
- Model identification
- Governance
  
--------------------------
- Group Model Risk Standards (GMRS): It state how things must be done to meet the requirements set out in the GMRP.
- Group Model Risk Policy (GMRP): It sets out requirements and responsibilities for the identification, measurement, and monitoring of Model Risk.
- Model Family Standards (MFS): Model Family Standards state how things must be done within a given model family
  
-------------------------
Model Risk Policy and Governance:
- Responsible for ongoing assessment of the model risk management framework
- Responsible for preparing model risk oversight reporting
- Responsible for the governance and execution of the annual status assessment for models
- Responsible for preparing Group’s model risk assessments and Group’s model risk profile reporting
  
-------------------

Third Party Model:
- Assessment of model development design and methodology
- Assessment of input data quality, security, privacy, integrity, bias and representativeness
- Assessment of any overlays or post model output adjustments
- Assessment of the Model Control Framework and Implementation
- Assessment of the Monitoring approach
- Assessment for Regulatory Compliance
- Model performance testing on recent out of time data
- Review of the model change management process
- Validation team to review the validation conducted by the vendor and document the key findings and recommendations in the model approval request template.
- Vendor-model: outcomes analysis, sensitivity analysis, benchmarking, monitoring.  
----------------------------------------------------------------------------------------------------
 
### Check Bias and Variance:

https://github.com/amitmse/in_Python_/blob/master/Others/README.md#bias---variance
  
#### To check above performance: 
- Cross-Validation: It reveals model's performance is consistent across different data samples, indicating lower variance. It also helps identify if the model's assumptions are too restrictive, leading to high bias.  
- Learning Curves: Plot the training and validation errors against the size of the training dataset. 
- Interpreting the curves:
	- High Bias (Underfitting): If both training and validation errors are high and close together, the model may be too simple and not capturing the underlying patterns in the data. High Training Error, High Validation Error: Suggests underfitting (high bias).
	- High Variance (Overfitting): If the training error is low but the validation error is high, the model is overfitting the training data and not generalizing well to unseen data Low Training Error, High Validation Error: Indicates overfitting (high variance).
	- Sensitivity Analysis: Test the model's sensitivity to changes in input variables.
   
---------------------------------------------------------------------------------------------------

#### Check model metrics: 

https://github.com/amitmse/in_Python_/blob/master/Others/README.md#model-metrics

- accuracy, precision, recall, F1-Score, and error rates (MAE, MSE, RMSE).
- Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values. 
- Mean Absolute Error (MAE): Measures the average absolute difference between predicted and actual values.
- Root Mean Squared Error (RMSE): It helps interpret errors in the same units as the target variable. It follows an assumption that errors are unbiased and follow a normal distribution. 
- Bias towards the Majority Class, Actual Performance of the Minority Class.
- Below metrics help to indentify:
- F1-Score: Provides a harmonic mean of precision and recall, accounting for both metrics and balancing their importance. Why harmonic mean and not an arithmetic mean: HM punishes extreme values.
- G-Mean: Computes the geometric mean of sensitivities for each class, providing a better overall picture of performance across all classes.  
   
----------------------------------------------------------------------------------------------------
    
#### Model Interpretability and Explainability in Validation:
 
- Interpretability: It helps identify potential model weaknesses, fostering robustness and reliability. credit-scoring model relies too heavily on a single variable, leading to biased decisions.
- Explainability: It builds trust by shedding light on the factors driving the model's decisions. why certain financial behaviors contribute more to the model's risk assessment.
- To provide explanations for complex models like neural networks, RF and GBM use methods like:

- Feature Importance: https://github.com/amitmse/in_Python_/blob/master/Others/README.md#feature-importance
- SHapley: https://github.com/amitmse/in_Python_/blob/master/Others/README.md#shapley-shapley-additive-explanations
- LIME: https://github.com/amitmse/in_Python_/blob/master/Others/README.md#lime-local-interpretable-model-agnostic-explanations
- PDPs: https://github.com/amitmse/in_Python_/blob/master/Others/README.md#partial-dependence-plots-pdps
- ALE: https://github.com/amitmse/in_Python_/blob/master/Others/README.md#accumulated-local-effects-ale-plots

--------------------------------------------------------------------------------------------------------

# ML Model Issue:

1. A high-performing model can still be completely wrong: Instead of a logical monotonic relationship (higher credit score → lower default risk), the model learned a non-monotonic pattern.

2. Hidden Model Weaknesses Lurking Beyond Aggregate Metrics: Failure clustering analysis uncovered a major issue—model performance is not homogeneous across input segments. Some clusters exhibit large mean absolute residuals (especially cluster 0) meaning our model struggles significantly in certain regions of the input space.

3. Harmful Side Effects of Variables: credit score is not only influencing model predictions significantly but is also a primary driver of model errors (particularly, for medium and low scores). Its interaction with credit utilization further amplifies these errors, as shown clearly by the main effect and interaction plots. When a variable strongly impacts both model outputs and errors, it's a flashing red flag that your model might be misaligned, unstable, or missing critical interactions. Ignoring this could mean you're building on shaky ground.

4. Segment-level Miscalibration: Machine learning models, like XGBoost, often struggle with probability miscalibration—particularly within specific segments of your data. Proper calibration—using techniques like Platt Scaling, Isotonic Regression, or Venn-Abers Prediction—is not merely beneficial; it's essential for critical tasks such as credit default prediction, where accurate probabilities directly influence business decisions.

5. Performance Fragility--Weak Resilience against Distribution Drift: Performance fragility due to distribution drift can significantly impact the effectiveness of models in production environments. Distribution drift occurs when the statistical properties of the input data change over time This fragility manifests as deteriorating prediction performance when model performance is not homogeneous across input clusters. From the observations related to Holes #2, #3, and #4, we recognize that model performance is not uniform across different data segments. Such non-homogeneity translates into fragility during distribution drift in production. To anticipate and mitigate the risk associated with distribution drift, it is essential to simulate various drift scenarios (https://lnkd.in/eAfngeAA). Model resilience testing involves creating synthetic drift conditions that the model may encounter in real-world applications as shown in the Figures below. Measuring feature distribution drift is pivotal in understanding how drift impacts model performance. Various metrics can be employed to quantify feature drift and assess model stability.
		
	- Kolmogorov-Smirnov (KS):The KS test compares the cumulative distributions of two datasets and identifies any significant differences. It helps quantify how the distribution of features has altered over time.
       
	- Wasserstein Distance: The Wasserstein Distance, also known as Earth Mover's Distance, measures the minimum amount of work required to transform one distribution into another. It provides a clear indication of feature distribution shifts.
	
	- Jensen-Shannon Distance (Population Stability Index, PSI): The Jensen-Shannon Distance is a symmetric measure of divergence between two probability distributions. The Population Stability Index (PSI) is  frequently used to assess the stability of features and detect drift over time. By proactively assessing these metrics, valuable insights are gained into which features are most vulnerable to drift and how significantly model performance could deteriorate in production. Conducting a feature vulnerability analysis helps pinpoint features susceptible to drift. By understanding these vulnerabilities, strategies can be devised to bolster model resilience and monitoring plan to mitigate risks. 

6. Silent Uncertainty: Understanding where your model is uncertain isn’t just a technical curiosity — it’s a necessity. Uncertainty Is a Signal: high predictive uncertainty indicates model unreliability in specific regions of the input space. These are areas where the model is unsure — and decisions based on its outputs are riskier. Find Risky Zones in the Input Space (see https://lnkd.in/eMSQkxWd): use conformal prediction intervals across many test samples to locate regions of high uncertainty. These are typically:
	- Sparse in the training data
	- Contain overlapping class distributions
	- Have inconsistent feature patterns
 
	Conformal prediction gives statistically valid uncertainty intervals around predictions. It allows us to say: “With 90% confidence, the true value lies within this range.” When those ranges are wide, the model is less certain. When they’re tight, the model is confident. In our credit model example (see Figures below), a few clusters (0 and 7) have high uncertainty with 90% confidence, the decisions are both default and non default (width == 2: decision is both classes). Clusters 4, 5 and 8 have higher uncertainty as well. In these regions, our model is unsure whether they’ll repay or default — and business risk increases.
  
	What to Do About It?
	- Flag high-uncertainty cases for manual review
	- Use uncertainty to prioritize retraining data collection
	- Adjust decision thresholds or introduce fallback rules in uncertain areas
	- Build explainability reports showing which input features (e.g., Score, DTI, Utilization) drive high uncertainty
   
	It’s Not Just About Accuracy: accuracy alone is misleading. What matters is how confident your model is when making decisions — especially in regulated, high-stakes settings like lending or healthcare.

7. The Noise Trap—Threat of "Benign Overfitting" : One of the most overlooked challenges in machine learning is lack of robustness due to benign overfitting. Models often look great in development—where the train and test sets come from the same distribution—but run into trouble in production when the input noise or data distribution changes. The result? A rapid performance drop that no one saw coming. This figure illustrates the problem:   
	- Perturbed Model Performance (Top-Left): Notice how the AUC drops significantly under noise perturbations. Small changes in inputs can cause large swings in performance—classic fragility.
	- Cluster Residual (Top-Right): Clusters 0 and 8 stand out as the worst in terms of robustness, indicating these segments of the data are especially sensitive to noise.
	- Feature Importance (Bottom-Left): We see which features drive the fragility. “Score,” “Utilization,” and “DTI” are among the top factors contributing to the model’s noise sensitivity.
	- Density Comparison (Bottom-Right): This plot highlights the problem are from Cluster 8. A shift to mid score threatens model robustness. 
Key Takeaways:
	- Benign Overfitting can mask true risk when train and test data share the same distribution.
	- Production Noise often differs from development, triggering unexpected performance declines.
	- Identifying Fragile Clusters (like clusters 0 and 8 here) is crucial to pinpoint where the model needs improvement.
	- Understanding Feature Drivers of robustness problems (e.g., “Score,” “Utilization,” “Income”) helps us prioritize feature engineering and model tuning.

	Robustness testing—especially under varying noise conditions—is essential to ensure your model doesn’t crumble when faced with real-world data. By diagnosing where and why a model is overly sensitive, you can shore up these “holes” and build a more stable foundation for long-term success.
      
8. Heterogeneity Blindspots—Masked Hidden Diversity: One of the most critical pitfalls in machine learning is assuming that a single, monolithic model can capture all the nuances and complexities of real-world data without segmentation. In reality, data is inherently heterogeneous, and overlooking this diversity can lead to spurious relationships, poor performance, and vulnerabilities to distribution drift.

	Consider our credit modeling example: distinct sub-populations exhibit unique characteristics and risk drivers. This means that: Unconstrained Single Models: They may inadvertently capture nonsensical effects in certain clusters.

	Monotonically Constrained Models: While they enforce logical behavior, they can be too rigid to adapt to the data’s diverse nature.

	Mixture of Experts (MoE) Framework: This approach allows each “expert” to specialize in a specific sub-population, uncovering hidden diversity and leading to models that are more resilient, interpretable, and conceptually sound. See Feature Importance plots of two very distinct sub-populations below. 

	Revealing hidden diversity and addressing heterogeneity in the population is not only boosts overall performance but also enhances performance uniformity, making the model more resilient against distribution drift. For instance, in the figure provided, Cluster 0 the worst-performing region under a single model—shows marked improvement when modeled using MoE. Even the "worst" expert in the MoE framework outperforms many segments of the single model.

	Mixture of Experts is far more sophisticated than simple segmentation. While segmentation typically divides the data into static groups, MoE employs a dynamic gating mechanism that assigns varying weights to different expert models based on the input features. This adaptive process allows the model to capture subtle, continuous variations in data heterogeneity, handle overlapping regions, and respond to changes in the data distribution. Instead of treating each segment as completely independent, MoE enables experts to collaborate—learning how to optimally combine their predictions for each specific input. This results in a more expressive and flexible modeling framework that uncovers hidden diversity and significantly enhances overall performance. 

------------------------------------------------------------------------------------------------------------------------

### Hyperparameters

- Validation is crucial for selecting the best model, tuning hyperparameters, and ensuring the model can adapt to new situations. It is the process of finding the best set of hyperparameters for a model to maximize its performance.
- Refer to model page for the detail

-------------------------
#### Grid Search, Randomized Search or Bayesian Optimization

- Use techniques like Grid Search, Randomized Search or Bayesian Optimization to explore the parameter space and find the optimal combination.
    
- Grid Search: Defines a grid of hyperparameter values and tests all possible combinations. 
	- Pro: Simple to understand and implement. Finding the best combination within the defined grid.
	- Cons: Computationally expensive, especially with many hyperparameters or large grids. May miss the optimal hyperparameters if the grid is not fine-grained enough.
   
- Random Search: Randomly samples hyperparameters from a defined range or distribution, without evaluating all combinations.
	- Pros: More efficient than grid search, especially with high-dimensional hyperparameter spaces. Can find good hyperparameters with fewer evaluations. 
	- Cons: May not find the optimal hyperparameters, especially if the search space is large. Requires careful selection of the number of iterations to balance exploration and efficiency.
   
- Bayesian Optimization: Uses a probabilistic model (typically a Gaussian process) to learn the relationship between hyperparameters and performance. It then intelligently explores the hyperparameter space, focusing on regions with the highest probability of containing the optimal hyperparameters. 
	- Pros: Efficient than grid and random search, requirs fewer evaluations to find good hyperparameters. Can handle non-convex and noisy objective functions. Can be parallelized to speed up the search process. 
	- Cons: Can be computationally expensive for each iteration, especially with complex models. Requires a suitable Gaussian process model and careful selection of hyperparameters for the model.
 
--------------------------------------------------------------------------------------------------------------------------

# Model Metrics

- beta(x) 	= covariance(x,y) / variance(x)
- correlation(x,y)= covariance(x,y) / [variance(x) * variance(y)]

----------------------------------------------------------------------------

- TSS 		= SUM[y-mean(y)]^2
- RSS 		= SUM[y-predicted(y)]^2
- R Squared	= 1.0 - (RSS/TSS)
- VIF 		= 1.0 / (1.0 - R Squared)

----------------------------------------------------------------------------

- AIC		= ( No of variable * 2)             - ( 2 * -Log Likelihood )
- BIC		= { No of variable * log(No of obs)}  - ( 2* -Log Likelihood )
- Gini/Somer’s D = [2AUC-1] OR [(Concordant - Disconcordant) / Total  pairs]
- Divergence 	= [(meanG – meanB)^2] / [0.5(varG + varB)]      [meanG = mean of score only for good, varB= variance of score only for bad ]

----------------------------------------------------------------------------

- True Positives (TP) = Correctly Identified
- True Negatives (TN) = Correctly Rejected
- False Positives (FP) = Incorrectly Identified = Type I Error
- False Negatives (FN) = Incorrectly Rejected	= Type II Error

----------------------------------------------------------------------------

- Recall and Precision:
	- Recall (Sensitivity) = Ability of the classifier to find positive samples from all positive samples
	- Recall is the fraction of instances that have been classified as true. On the contrary, precision is a measure of weighing instances that are actually true.
	- Recall/Sensitivity: how many relevant instances are selected.

	- Precision = Ability of the classifier not to label as positive a sample that is negative (positive predictive value)	
	- While recall is an approximation, precision is a true value that represents factual knowledge.
	- Precision/Specificity: how many selected instances are relevant.

----------------------------------------------------------------------------

- Specificity = Measures the proportion of actual negatives that are correctly identified (true negative rate)

![Function](https://github.com/amitmse/in_Python_/blob/master/Formula/Confusion%20Matrxi.jpg)

- True Positive Rate / Sensitivity / Recall : 	TP  / (TP + FN) = TP / Actual Positives
- True Negative Rate / Specificity : 	    	TN  / (TN + FP) = TN / Actual Negatives
- False Positive Rate / Type I Error: 	    	FP  / (FP + TN) = FP / Actual Negatives = 1 - Specificity
- False Negative Rate / Type II Error : 	FN  / (FN + TP) = FN / Actual Positives = 1 - True Positive Rate

- Positive Predictive Value / Precision :    	TP  / (TP + FP)
- Negative Predictive Value : 		   	TN  / (TN + FN)
- False Discovery Rate: 			FP  / (FP + TP) = 1 - Positive Predictive Value
  
-------------------------------------------------------------------------------------------

- F1-Score :

	- F1-Score = 2*TP/ (2TP + FP + FN)   =   [2 * (Precision * Recall) / (Precision + Recall)]

	- What it is:
 		- The F1 score is a metric that balances precision and recall, which are both important in evaluating a model's performance.
		- F1 score (also F-score or F-measure) is a measure of a test's accuracy.
 		- The F1-score gives you the harmonic mean of precision and recall.
		- The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.
		- The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.
		- It considers both the precision and the recall of the test to compute the score: 
			- precision is the number of correct positive results divided by the number of all positive results returned by the classifier. precision is the measure of how accurate the classifier’s prediction of a specific class
			- recall is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). recall is the measure of the classifier’s ability to identify a class.
		- If the classifier predicts the minority class but the prediction is erroneous and false-positive increases, the precision metric will be low and so as F1 score. Also, if the classifier identifies the minority class poorly, i.e. more of this class wrongfully predicted as the majority class then false negatives will increase, so recall and F1 score will low. F1 score only increases if both the number and quality of prediction improves. F1 score keeps the balance between precision and recall and improves the score only if the classifier identifies more of a certain class correctly.
		
	- How to interpret:
 		- Precision is the percentage of positive predictions that were actually correct (true positives / total positive predictions).
		- Recall (also known as sensitivity) is the percentage of actual positive cases that the model correctly identified (true positives / total actual positives).
		- F1 score is the harmonic mean of precision and recall, providing a single score that considers both.
		- Why harmonic mean and not an arithmetic mean. This is because HM punishes extreme values more.

	- Use Cases: F1 score is particularly useful in scenarios where both precision and recall are important, or when dealing with imbalanced datasets.

-----------------------------------------------------------------------------------

- Accuracy:
	- Accuracy = (TP + TN)/ (TP  + TN + FP + FN)  
	- What it is: Accuracy is the percentage of predictions that the model gets right, calculated as (correct predictions / total predictions) * 100.
	- How to interpret: A higher accuracy means the model is making more correct predictions overall.
	- Use Cases: Accuracy is a good general metric, but it can be misleading in imbalanced datasets, as a model might achieve high accuracy by simply predicting the majority class. 

	- Accuracy paradox:
		- if the incidence of category A is dominant, being found in 99% of cases, then predicting that every case is category A will have an accuracy of 99%. Precision and recall are better measures in such cases.
		- The underlying issue is that there is a class imbalance between the positive class and the negative class.
		- Prior probabilities for these classes need to be accounted for in error analysis.
		- Precision and recall help, but precision too can be biased by very unbalanced class priors in the test sets.

	- Accuracy is not appropriate when the data is imbalanced. Because the model can achieve higher accuracy by just predicting accurately the majority class while performing poorly on the minority class which in most cases is the class we care about the most.

-----------------------------------------------------
   
- Receiver operating characteristic (ROC) curve:
	- Receiver Operating Characteristic is a measurement of the True Positive Rate (TPR) against False Positive Rate (FPR).
	- True Positive (TP) as TPR = TP/ (TP + FN). On the contrary, 
	- False positive rate is determined as FPR = FP/FP+TN
		where where TP = true positive, TN = true negative, FP = false positive, FN = false negative.

	- Model's ability to distinguish between classes. The biggest advantage of using the ROC curve is that it is independent of the change in the proportion of positive class. It considers the predicted probabilities for determining our model’s performance.
	- Issue: it only takes into account the order of probabilities, and does not take into account the model’s capability to predict a higher probability for samples more likely to be positive.
	- The ROC curve is the plot between sensitivity and (1- specificity).

-----------------------------------------------------

- AUC (Area Under the Curve) / C statistics / :
	- AUC = Percent Concordant + 0.5 * Percent Tied 
	- What it is:
		- AUC represents the area under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate (how well the model identifies positives) against the false positive rate (how often the model incorrectly identifies negatives as positives) at various thresholds.
		- Trade-off between the true positive rate (TPR) and the false positive rate (FPR). Advantage of ROC curve is that it is independent of the change in the proportion of responders.
		- The ROC curve is a graphical plot that illustrates the performance of any binary classifier system as its discrimination threshold is varied.
		- True positive rate (Sensitivity : Y axis ) is plotted in function of the false positive rate (100-Specificity : X axis) for different cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold.

	- How to interpret: An AUC of 1 means the model perfectly distinguishes between the two groups. An AUC of 0.5 means the model is no better than random guessing. A higher AUC indicates better model performance. 
	- Use Cases: AUC is particularly useful for evaluating models in scenarios with imbalanced datasets (where one group is much larger than the other) because it considers the trade-off between true and false positives.

	https://www.geeksforgeeks.org/how-to-handle-imbalanced-classes-in-machine-learning/

-----------------------------------------------------

- Accuracy vs ROC AUC:
	- First difference is that you calculate accuracy on the predicted classes while you calculate ROC AUC on predicted scores. That means you will have to find the optimal threshold for your problem. 
	- Secondly, accuracy scores start at 0.93 for the very worst model and go up to 0.97 for the best one.
	- Remember that predicting all observations as majority class 0 would give 0.9 accuracy, so our worst experiment, BIN-98 is only slightly better than that.
	- Yet the score itself is quite high, and it shows that you should always take an imbalance into consideration when looking at accuracy. 

-----------------------------------------------------

- AUC vs ROC:
	- AUC curve is a measurement of precision against the recall. Precision = TP/(TP + FP) and TP/(TP + FN).
	- This is in contrast with ROC that measures and plots True Positive against False positive rate.

-----------------------------------------------------

- F1 score vs ROC AUC:
	- One big difference between the F1 score and the ROC AUC is that the first one takes predicted classes, and the second takes predicted scores as input.
 	- Because of that, with the F1 score, you need to choose a threshold that assigns your observations to those classes. Often, you can improve your model performance a lot if you choose it well.
 
-----------------------------------------------------

- F1 score vs Accuracy:
	- F1 score balances precision and recall in the positive class, while accuracy looks at correctly classified observations, both positive and negative.
	- That makes a big difference, especially for the imbalanced problems, where by default our model will be good at predicting true negatives and hence accuracy will be high.
 	- However, if you care equally about true negatives and true positives, then accuracy is the metric you should choose. 

-----------------------------------------------------------------------------------

- Gini Coefficient:
	- What it is: The Gini coefficient is derived from the AUC and represents the model's ability to discriminate between the two groups.
 		Gini is nothing but the ratio between the area between the ROC curve and the diagonal line & the area of the above triangle.
   
	- How to interpret: Gini = 2 * AUC - 1. 
		A Gini of 0 means the model is no better than random guessing.
		A Gini of 1 means the model perfectly distinguishes between the two groups.

	- Use Cases: The Gini coefficient provides a simple and intuitive measure of model performance, especially for non-technical audiences. 

---------------------------------------------------------------------------------------------------------------

- Kolmogorov-Smirnov (KS) Test:
	- What it is: The KS test is a statistical test used to determine if two distributions are significantly different. KS measures the degree of separation between the positive and negative distributions.
 		The K-S is 100 if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives.
		If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The KS would be 0.

	- How to interpret: In the context of machine learning, the KS test can be used to evaluate the performance of a model by comparing the distribution of predicted probabilities with the distribution of actual outcomes.
	  A higher KS value indicates a greater difference between the two distributions, suggesting that the model is better at distinguishing between the two groups.
   
	- Use Cases: The KS test is useful for evaluating the performance of a model in a more nuanced way than AUC or Gini, as it considers the entire distribution of predicted probabilities. 

---------------------------------------------------------------------------------------------------------------

- Gain and Lift charts:
	- Check the rank ordering of the probabilities.
 	- This graph tells how well is model is segregating positive from negative.
  	- Lift is dependent on the total response rate of the population.
  	- Hence, if the response rate of the population changes, the same model will give a different lift chart.

---------------------------------------------------------------------------------------------------------------

- Standard Error Coef: 
	- Linear regression standard error of Coef : SE  = sqrt [ S(yi - yi)2 / (n - 2) ] / sqrt [ S(xi - x)2 ]
	- The standard error of the coefficient estimates the variability between coefficient estimates that you would obtain if you took samples from the same population again and again. 
	- The calculation assumes that the sample size and the coefficients to estimate would remain the same if you sampled again and again.
	- Use the standard error of the coefficient to measure the precision of the estimate of the coefficient. 
	- The smaller the standard error, the more precise the estimate.

---------------------------------------------------------------------------------------------------------------

# Feature Importance

- Analyze the importance of each input feature in the model's predictions. 
- Techniques like tree-based models or methods that calculate the importance of each feature based on its contribution to the model's predictions.
- Tree-Based Algorithms feature importance scores based on how much each feature reduces impurity (e.g., Gini index or information gain) in the decision tree nodes.
- Tree based: Decision Trees, Random Forests, XGBoost, LightGBM.
- Linear Models (Logistic Regression, Linear Regression) feature importance is derived from the coefficients of the linear model.
- Features with larger absolute coefficient values are considered more important.
- It is also known as the Gini importance. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.
  
- That reduction or weighted information gain is defined as. The weighted impurity decrease equation is the following:

	N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)
	
			N 	: Total number of samples
			N_t 	: No. of samples at the current node
			N_t_L 	: No. of samples in the left child 
			N_t_R 	: No. of samples in the right child

- Below are same as Feature Importance
	- SHAP: SHapley Additive exPlanations.
	- LIME: Local Interpretable Model-Agnostic Explanations. 
	Details are below in link
 - For Neural Networks Techniques, SHAP or feature importance (via gradient analysis) are used to understand the contribution of individual features.
 - Permutation Feature Importance: This technique measures the contribution of a feature by measuring the changes in the model performance after randomly shuffling its values.
  
------------------------------------------------------------------------------------------------------
  
# SHapley: SHapley Additive exPlanations. 

- It's a game-theoretic approach that calculates the average marginal contribution of each feature, helping to understand how each feature affects the model's output SHAP to provide overall (Global) explanations of how the model as a whole makes predictions, focusing on the overall influence of each feature.
- SHapley Values provide a way to understand how each feature contributes to the model's predictions. Similar to the beta of linear regression. It helps in identifying the most important features and understanding their impact on the model.
- This method uses random feature combinations as input and compute the changes in the model performance. The features which impact the performance the most are considered the most important ones.
- SHapley does not go on and retrain the model for each subset. Instead, for the removed or left out feature, it just replaces it with the average value of the feature and generates the predictions.
- SHapley attribute the difference between a model’s prediction for an instance and the average prediction to each feature.
	- Additivity: The sum of SHAP values for all features equals the difference between the prediction and the baseline (average prediction).
	- Local Accuracy: Each feature’s contribution is computed for a single instance.
	- Consistency: If a feature’s impact increases, its SHAP value won’t decrease.
   
- When to Use SHAP:
	- You need instance-level explanations (e.g., “Why was this loan rejected?”).
	- You want to compare feature importance across the dataset (aggregated SHAP).
	- Your model has nonlinear interactions best captured per-instance.
	- Use SHAP if you need to explain individual predictions or compare feature importance globally.

#### Step-by-step:
- Baseline: The SHAP value calculation starts with a baseline value, which represents the expected output of the model when no features are present. This is often the mean or median of the model's predictions across the training data.
- Combinations of Features: For each data point and feature, SHAP examines all possible combinations of features (coalitions) that could be present in the prediction.
- Marginal Contributions: For each coalition, the model's prediction is calculated with and without the specific feature in question. The difference between these predictions represents the marginal contribution of that feature to the prediction for that coalition.
- Shapley Values: The SHAP value for a feature is the weighted average of all its marginal contributions across all possible coalitions. This weighted average ensures that each feature's contribution is fairly distributed across all possible combinations of features.

SHapley value =  sum [weight * (prediction with feature - prediction without feature)]

	- weight assigned to a particular coalition based on the number of ways the feature could have joined the coalition.
	- prediction with feature is the model prediction when the feature is included in the coalition.
	- prediction without feature is the model prediction when the feature is excluded from the coalition.
 
![image](https://github.com/user-attachments/assets/e97c45c3-c565-473e-82ca-b2d11b95c244)

	- n is total number of features
	- N contains all the possible feature subsets not containing feature i
	- S is one feature set from N
	- v(x) is the trained model prediction function f(x), x is a model input instance
	- |S|is the number of not missing features in set S

-  Interpretion of SHAP Values: The SHAP values indicate the degree to which a feature influences the model's prediction. A positive SHAP value suggests that the feature contributes to a higher prediction, while a negative SHAP value suggests a lower prediction. The magnitude of the SHAP value reflects the strength of the feature's influence. The sum of SHAP values for all features equals the difference between the model prediction and the baseline prediction. If a feature has no impact on the prediction, its SHAP value will be zero. If a feature's impact changes when other features are included or excluded, the SHAP value reflects that change. E[f(X)] refers to the baseline (mean or median in the case of regression).  f(x) is the value predicted by our model. 

#### Limitations of SHapley

- The computational complexity of calculating SHAP values can be challenging, especially for models with a large number of features. To address this, approximation techniques like Kernel SHAP and Tree SHAP have been developed, but these methods may introduce some inaccuracies in the explanations.
- SHAP assumes feature independence, which may not always hold true in real-world datasets. The aggregation of individual SHAP values to provide global insights can also be a complex task, requiring careful interpretation and consideration of the underlying data and model characteristics.
-  Computationally expensive, especially with many features (where feature interactions are complex)

https://github.com/amitmse/in_Python_/tree/master/Boosting#shapley-additive-explanations-shap

------------------------------------------------------------------------------------------------------------

# LIME: Local Interpretable Model-Agnostic Explanations

- It creates a simplified, interpretable model to explain the predictions of a complex model.
- It provides Local Explanations for individual predictions, focusing on how each input feature contributes to that specific prediction.
- LIME creates synthetic input data around a specific instance and observing how the model predictions change. And then trains a surrogate model (e.g., a linear model) on synthetic data.
- LIME is much faster than SHAP. Shapley values take a long time to compute.
- LIME is actually a subset of SHAP but lacks the same propertie.
- LIME does not support functionality for global interpretation. LIME to interpret individual predictions locally, like SHAP.
  
#### Step-by-step:
1. Choose the specific data point for which want to understand the model prediction. 
2. Create a synthetic data, slightly altered around original data point.

		synthetic data = original data point * (1 + random factor)
 
	- Synthetic data is calculated by introducing controlled changes (noise or distortions) to original data points.
	- Noise: Adding random values (e.g., from a Laplace distribution) to the original data.
	- Data Transformation: Using techniques like SVD (Singular Value Decomposition) or geometric rotations to transform the data into a different space. 
	- Randomization: Shuffling categorical data, or adding random shifts to time values. 

3. Use model to predict the output for each synthetic data point.
4. Obtain predictions for synthetic data point using original model (ML). 
5. Assign weights to synthetic data based on the original data point, with closer samples receiving higher weights.
6. Train an interpretable model (like a linear model) on the synthetic data, using the weights calculated in the previous step. 
7. Analyze the coefficients or feature importances of the surrogate model to understand how each feature contributes to the prediction of the black-box model for the specific instance. 

#### Limitations of LIME
- Local vs. Global Interpretability: LIME provides explanations for individual predictions, which may not reflect the model's overall behavior.
- Linear Assumption: LIME assumes a linear relationship in the local approximation, which might oversimplify complex decision boundaries.
- Stability and Consistency: The random sampling process in LIME can lead to different explanations for the same input, affecting reliability.
- Sampling Bias: The synthetic data might not accurately represent real-world data distributions, potentially leading to biased explanations.
	
-------------------------------------------------------------------------------------------------------  

# Partial Dependence Plots (PDPs)

- Similar to sensitivity analysis.
- PDPs are a method to visualize, how individual features influence (marginal effect) model predictions.
- It helps interpret complex models by showing the average marginal effect of a feature on the predicted outcome, while holding other features constant.
- PDPs are calculated by averaging the model's predictions across all possible combinations of the other features, while varying the feature(s) of interest.

- Benefits:
	- Explainability: PDPs help make complex models more transparent and understandable, especially for non-technical audiences.
	- Feature Importance: They can reveal which features have the most significant impact on the model's predictions. 
	- Debugging: PDPs can help identify issues with the model, such as unexpected relationships between features and predictions
   
- Limitations:
	- PDPs can be biased in scenarios with highly correlated features, as they use marginal rather than conditional probabilities. Accumulated Local Effects (ALE) plots are an alternative that addresses this bias.
	- PDPs are best suited for visualizing the relationship between one or two features and the model's output. They become harder to interpret with more than two features.
	- PDPs only capture the marginal effect of a feature, potentially hiding complex interactions between multiple features.

------------------------------------------------------------------------------------------------------------

# Accumulated Local Effects (ALE) plots

- ALE plots are designed to overcome PDPs limitation (above). Instead of averaging over all observations globally, ALE focuses on differences in predictions to isolate each feature’s effect.
- Compute Local Derivatives: They first estimate the local effect (i.e., the derivative) of the feature on the model prediction in small intervals (bins) of the feature’s range.
- Accumulate the Effects: Then they integrate (accumulate) these local effects over the feature’s range, starting from a reference point (often the minimum value).
- Centering: Finally, the accumulated effect is centered so that the overall average effect is zero, making it easier to compare across features.
- When to Use ALE:
	- You care about global feature effects (e.g., “Does income positively affect loan approval?”).
	- Features are correlated, and you want reliable estimates.
	- You need a clear visual of the feature’s directional trend.
	- Overall directional relationship between a feature and the outcome, especially with correlated features.

--------------------------------------------------------------------------------------

- Accuracy: Measures the overall proportion of correct predictions. 
- Precision: Measures the proportion of true positives among all positive predictions.
- Recall (Sensitivity): Measures the proportion of actual positives that are correctly identified.
- Precision-Recall Curves: Visualize the trade-off between precision and recall across different thresholds, enabling a deeper understanding of the model's performance under various scenarios.
- Log Loss: Measures the performance of a classification model based on probability predictions. It's a negative average of the log of corrected predicted probabilities.
- R-squared (Coefficient of Determination): Measures the proportion of variance in the dependent variable that can be predicted from the independent variables.
- AUC measures how well a model distinguishes between two groups
- Accuracy is the percentage of correct predictions
- F1 Score balances precision and recall
- Gini is a measure of model discrimination derived from AUC
- Kolmogorov-Smirnov (KS) measures the difference between two distributions. 
- MCC: correlation coefficient between the observed and predicted binary classifications.

------------------------------------------------------------------------------------------------------

https://www.kaggle.com/discussions/getting-started/170389

-----------------------------------------------------------------------------------------------------------------------------------
	https://www.linkedin.com/posts/agus-sudjianto-76519619_does-your-model-have-more-holes-than-swiss-activity-7307963627938992128-K6uD/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_does-your-model-have-more-holes-than-swiss-activity-7308415223051223041-3o9a/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_does-your-model-have-more-holes-than-swiss-activity-7308807687390003201-4wX1/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_does-your-model-have-more-holes-than-swiss-activity-7309182083241816065-NiGG/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_hole-5-performance-fragility-weak-resilience-activity-7309521010972655616-zDH0/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_hole-6-silent-uncertaintywhich-parts-of-activity-7309959171263660032-wBkg/?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAPDob0BbqFMdl3Xolh4xcMoT3do--FKv3g
	
	https://www.linkedin.com/posts/agus-sudjianto-76519619_hole-7-the-noise-trapthreat-of-benign-activity-7310308433537171456-fIJE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAJdJL4BriNQhSVKeKIC1Ppfn_ZfLrujGCo

	https://machinelearningmastery.com/how-to-code-the-students-t-test-from-scratch-in-python/
	Stock-en-Presentation-Agri-test-CCAR-Work-Risk-FRM

-----------------------------------------------------------------------------------------------------------------------------------
